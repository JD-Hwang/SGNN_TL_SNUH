{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU number\n",
    "GPU = 1\n",
    "\n",
    "num_workers = 4\n",
    "\n",
    "#Random Seed\n",
    "seed = 19\n",
    "\n",
    "#Target set to 'p' since we're predicting the p-factor. \n",
    "target_name = 'p'\n",
    "\n",
    "smoothing = True\n",
    "gsr = False\n",
    "censor = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/hjd/IG_my_study/SNUH/data/CFA_factors_results/pretrain_model/SparsityGrid/p/Hsp:[0.95,0.3,0.3]_Maxb:[0.01, 0.05, 0.02]_Betalr:[0.001, 0.005, 0.002]_LR:[5e-05]_Act:[elu]_Opt:[nag]_DO:[0.9,0.9,0.0]_lambda:[0.01]_seed[42]/\n"
     ]
    }
   ],
   "source": [
    "#Pre-traned SGNN path\n",
    "prt_dir = \"/users/hjd/IG_my_study/SNUH/data/CFA_factors_results/pretrain_model/SparsityGrid/p/Hsp:[0.95,0.3,0.3]_Maxb:[0.01, 0.05, 0.02]_Betalr:[0.001, 0.005, 0.002]_LR:[5e-05]_Act:[elu]_Opt:[nag]_DO:[0.9,0.9,0.0]_lambda:[0.01]_seed[42]/\"\n",
    "print(prt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.24.1', '1.10.1+cu102')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn, torch\n",
    "sklearn.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index for hyperparameter selection.\n",
    "#Hyperparameter sets were selected with Paramgrid, then one specific hyperparameter was chosen based on this index.\n",
    "temp_sel_idx = 0 \n",
    "\n",
    "#Number of cross-validation. 5 as default\n",
    "n_cv = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Fold: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "Selected Fold: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "sel_cv_idx = 0\n",
    "jump_val = n_cv\n",
    "outer_cv_part = np.arange(sel_cv_idx * jump_val, sel_cv_idx * jump_val + jump_val)\n",
    "print(\"Total Fold: {}\".format(outer_cv_part))\n",
    "\n",
    "select_fold = [0,1] # #has to be list format\n",
    "print(\"Selected Fold: {}\".format(select_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "act_func_name = \"elu\"\n",
    "optimizer_name = \"nag\"\n",
    "\n",
    "#Number of nodes in NN. 1024 as default\n",
    "h1_cand = [1024]\n",
    "h2_cand = [1024]\n",
    "\n",
    "#Dropout rate\n",
    "dropout_h1_cand = [0.3]\n",
    "dropout_h2_cand = [0.3]\n",
    "\n",
    "#Batch size / Learning rate / LR patience for scheduling / LR scaling ratio / Epochs / Freezing epochs\n",
    "batch_size_cand = [2]\n",
    "lr_cand = [1e-05]\n",
    "lr_patience_cand = [5]\n",
    "lr_factor_cand = [0.5]\n",
    "epochs_cand = [50]\n",
    "\n",
    "#Hoyer's sparsity candidates\n",
    "hsp_h1_cand = [0.95]\n",
    "hsp_h2_cand = [0.3]\n",
    "\n",
    "#L2 norm\n",
    "l2_param_cand = [2e-01]\n",
    "\n",
    "#Pretraining & layer freezing option\n",
    "pretrain_cand = [True]\n",
    "trainable_ext_cand = [False]\n",
    "trainable_prd_cand = [True]\n",
    "\n",
    "param_cand = {\n",
    "    \"h1\": h1_cand, \"h2\": h2_cand,  \n",
    "    \"dropout_h1\": dropout_h1_cand, \"dropout_h2\": dropout_h2_cand,\n",
    "    \"batch_size\": batch_size_cand, \"lr\": lr_cand, \"epochs\": epochs_cand, \n",
    "    \"lr_patience\": lr_patience_cand, \"lr_factor\": lr_factor_cand,\n",
    "    \"hsp_h1\": hsp_h1_cand, \"hsp_h2\": hsp_h2_cand, \"l2_param\": l2_param_cand,\n",
    "    \"pretrain\": pretrain_cand, \n",
    "    \"freeze_ext\": trainable_ext_cand, \"freeze_prd\": trainable_prd_cand\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime as dt\n",
    "from decimal import Decimal\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from pytz import timezone\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from PIL import Image\n",
    "import imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.autograd import Function, Variable\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau,CosineAnnealingWarmRestarts\n",
    "from torch.optim.swa_utils import SWALR, AveragedModel\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "nowtime = dt.now(timezone(\"Asia/Seoul\")); year = str(nowtime.year)[2:]\n",
    "month = '0{}'.format(nowtime.month) if nowtime.month < 10 else str(nowtime.month)\n",
    "day = '0{}'.format(nowtime.day) if nowtime.day < 10 else str(nowtime.day)\n",
    "hour = '0{}'.format(nowtime.hour) if nowtime.hour < 10 else str(nowtime.hour)\n",
    "minute = '0{}'.format(nowtime.minute) if nowtime.minute < 10 else str(nowtime.minute)\n",
    "sec = '0{}'.format(nowtime.second) if nowtime.second < 10 else str(nowtime.second)\n",
    "msec = str(nowtime.microsecond)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input / Target setting & Fold division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.826154 -1.5008454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(125, 61776)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X =np.load(f\"/data4/SNU/data/snuh_fc_temp_hjd/RSFC_Smoothing[{smoothing}]_GSR[{gsr}]_Censor[{censor}].npz\")[\"X\"]\n",
    "print(X.max(),X.min())\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(125, 61776) (125, 1)\n"
     ]
    }
   ],
   "source": [
    "y_df = pd.read_csv(\"/data4/SNU/data/demo_for_rsfc_df.csv\", index_col=0)\n",
    "cfa_fs = np.load(\"/data4/SNU/data/SNUH_CFA_5factor.npz\")[target_name]\n",
    "y_df['fs'] = cfa_fs\n",
    "\n",
    "# #Exclude outlier\n",
    "# exclude_sbj = y_df.loc[(y_df['fs']>1.5)&(y_df['group']=='CON'),'new_id'].tolist()[0]\n",
    "# exclude_sbj_idx = np.where(y_df['new_id'] == exclude_sbj)[0][0]\n",
    "# y_df = y_df.loc[y_df['new_id']!=exclude_sbj,:]\n",
    "# X = np.delete(X,exclude_sbj_idx,0)\n",
    "\n",
    "y = y_df['fs'].values.reshape(-1, 1)\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 100, 25)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "\n",
    "outer_n_splits = n_cv\n",
    "\n",
    "outer_train_folds_idx = []\n",
    "outer_test_folds_idx = []\n",
    "\n",
    "outer_skf = ShuffleSplit(\n",
    "    n_splits=outer_n_splits, test_size=0.20, random_state=seed)\n",
    "\n",
    "for n_outer, (outer_train_idx, outer_test_idx) in enumerate(\n",
    "    outer_skf.split(X, y)):\n",
    "    outer_train_folds_idx.append(outer_train_idx)\n",
    "    outer_test_folds_idx.append(outer_test_idx)\n",
    "    \n",
    "    X_outer_train, y_outer_train = X[outer_train_idx], y[outer_train_idx]\n",
    "\n",
    "\n",
    "len(outer_train_folds_idx),len(outer_train_folds_idx[0]), len(outer_test_folds_idx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Real-time visualization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(352,)\n",
      "['AUD', 'CON', 'CPAR', 'DAN', 'DMN', 'FPN', 'RSP', 'SAL', 'SCN', 'SMH', 'SMM', 'VAN', 'VIS', 'NONE']\n",
      "(352, 352)\n"
     ]
    }
   ],
   "source": [
    "num_ROIs = tot_rois = 352\n",
    "threshold = 1.96\n",
    "\n",
    "# Preparing draw feature map\n",
    "parcels = pd.read_excel(\"/users/hjw/data/ABCD/Parcels/Parcels.xlsx\", engine=\"openpyxl\")\n",
    "networks = list(parcels[\"Community\"]) + 19 * [\"Subcortex\"]\n",
    "\n",
    "networks_df = pd.DataFrame(networks, columns=[\"network\"])\n",
    "networks_df[networks_df[\"network\"] == \"Auditory\"] = \"AUD\"\n",
    "networks_df[networks_df[\"network\"] == \"Visual\"] = \"VIS\"\n",
    "networks_df[networks_df[\"network\"] == \"VentralAttn\"] = \"VAN\"\n",
    "networks_df[networks_df[\"network\"] == \"Subcortex\"] = \"SCN\"\n",
    "networks_df[networks_df[\"network\"] == \"Salience\"] = \"SAL\"\n",
    "networks_df[networks_df[\"network\"] == \"SMmouth\"] = \"SMM\"\n",
    "networks_df[networks_df[\"network\"] == \"SMhand\"] = \"SMH\"\n",
    "networks_df[networks_df[\"network\"] == \"RetrosplenialTemporal\"] = \"RSP\"\n",
    "networks_df[networks_df[\"network\"] == \"None\"] = \"NONE\"\n",
    "networks_df[networks_df[\"network\"] == \"FrontoParietal\"] = \"FPN\"\n",
    "networks_df[networks_df[\"network\"] == \"DorsalAttn\"] = \"DAN\"\n",
    "networks_df[networks_df[\"network\"] == \"Default\"] = \"DMN\"\n",
    "networks_df[networks_df[\"network\"] == \"CinguloParietal\"] = \"CPAR\"\n",
    "networks_df[networks_df[\"network\"] == \"CinguloOperc\"] = \"CON\"\n",
    "\n",
    "networks = np.array(networks_df[\"network\"]).astype(\"str\")\n",
    "network_label = np.unique(networks, return_index=True)[0].astype(\"str\")\n",
    "orig_network_path = \"/users/hjw/data/ABCD/npz_files/Gordon_network_labels.npz\"\n",
    "orig_networks = np.load(orig_network_path)[\"networks\"]\n",
    "print(orig_networks.shape)\n",
    "\n",
    "# Set order of network label \n",
    "new_orig_network_idx_order = [\n",
    "    'AUD', 'DAN', 'CPAR', 'NONE', \n",
    "    'SAL', 'VIS', 'RSP', 'DMN',\n",
    "    'SMM', 'CON', 'SCN', \n",
    "    'SMH', 'VAN', 'FPN'\n",
    "]\n",
    "\n",
    "sorted_order = sorted(new_orig_network_idx_order)\n",
    "sorted_order.remove(\"NONE\")\n",
    "sorted_order.append(\"NONE\")\n",
    "new_orig_network_idx_order = sorted_order\n",
    "print(new_orig_network_idx_order)\n",
    "\n",
    "new_orig_network_order = {\n",
    "    key:value for (key, value) in \n",
    "    zip(new_orig_network_idx_order, (np.arange(len(new_orig_network_idx_order))))\n",
    "}\n",
    "\n",
    "\n",
    "ref_orig_wfm = np.zeros((tot_rois, tot_rois))\n",
    "print(ref_orig_wfm.shape)\n",
    "ref_orig_wfm = pd.DataFrame(ref_orig_wfm, index=orig_networks, columns=orig_networks)\n",
    "ref_orig_wfm = ref_orig_wfm.sort_index(key=lambda x: x.map(new_orig_network_order), \n",
    "                                       axis=0)\n",
    "ref_orig_wfm = ref_orig_wfm.sort_index(key=lambda x: x.map(new_orig_network_order), \n",
    "                                       axis=1)    \n",
    "\n",
    "network_unq = new_orig_network_idx_order\n",
    "sorted_networks = np.array(ref_orig_wfm.columns, dtype=np.str)\n",
    "\n",
    "\n",
    "sorted_networks_df = pd.DataFrame(np.unique(sorted_networks, return_index=True)).T\n",
    "sorted_networks_df.columns = [\"networks\", \"n\"]\n",
    "sorted_networks_df = sorted_networks_df.sort_values(\n",
    "    by=\"networks\", key=lambda x: x.map(new_orig_network_order)\n",
    ")\n",
    "\n",
    "start_network_idx = np.array(sorted_networks_df.n)\n",
    "next_network_idx = np.hstack((start_network_idx[1:], 352))\n",
    "\n",
    "network_mid_idx = np.array((start_network_idx + next_network_idx) / 2, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wfm(vec):\n",
    "    wfm = np.zeros((tot_rois, tot_rois))\n",
    "    iu_non_di_idx = np.mask_indices(tot_rois, np.triu, 1)\n",
    "    wfm[iu_non_di_idx] = vec\n",
    "    il_idx = np.tril_indices(tot_rois, -1)\n",
    "    wfm[il_idx] = wfm.T[il_idx]\n",
    "    wfm_df = pd.DataFrame(wfm, index=orig_networks, columns=orig_networks)\n",
    "    wfm_df = wfm_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=0)\n",
    "    wfm_df = wfm_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=1)\n",
    "    \n",
    "    return wfm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input으로 model, epoch받아서 wfm 저장시키기\n",
    "\n",
    "def visualize_wfm(trained_model,epoch,epoch_gap = 10,threshold=False,mode = 'sum'):\n",
    "    \n",
    "    if epoch%epoch_gap !=0:\n",
    "        return\n",
    "    \n",
    "    wfm_save_dir = outer_save_dir+'/WFM/'\n",
    "    if not os.path.isdir(wfm_save_dir):\n",
    "        os.mkdir(wfm_save_dir)\n",
    "    \n",
    "    w_ext = []\n",
    "    w_reg = []\n",
    "    w_dsc = []\n",
    "\n",
    "    ext_hidden = trained_model.ext_1.weight.shape[0]\n",
    "    prd_hidden = trained_model.prd_1.weight.shape[0]\n",
    "\n",
    "    w = []\n",
    "\n",
    "    for name,params in trained_model.named_parameters():\n",
    "        if \"weight\" in name and \"bn\" not in name:\n",
    "            if \"ext\" in name or \"prd\" in name:\n",
    "                w.append(params)\n",
    "\n",
    "    w_ext_1 = w[0].detach().cpu().numpy().T\n",
    "    w_reg_1 = w[1].detach().cpu().numpy().T\n",
    "    w_reg_2 = w[2].detach().cpu().numpy().T\n",
    "\n",
    "    temp_w_ext = w_ext_1\n",
    "    temp_w_reg = np.matmul(np.matmul(temp_w_ext, w_reg_1), w_reg_2)\n",
    "\n",
    "    w_ext.append(temp_w_ext) #ext x pred1 (352x1024)\n",
    "    w_reg.append(temp_w_reg) #ext x pred1 x pred2 (352x1)\n",
    "\n",
    "#     np.savez(wfm_save_dir+\"/wfm_reg_epoch{}\".format(epoch), X=w_reg)\n",
    "\n",
    "    wfm_df = make_wfm(stats.zscore(w_reg[0].reshape(-1,)))\n",
    "\n",
    "    cols = rows = wfm_df.columns.values\n",
    "#     new_orig_network_idx_order = np.unique(cols)\n",
    "    avg_df_1 = pd.DataFrame(columns=cols, index=rows)\n",
    "    avg_df_2 = pd.DataFrame(columns=new_orig_network_idx_order, index=new_orig_network_idx_order)\n",
    "    avg_df_3 = pd.DataFrame(columns=new_orig_network_idx_order, index=new_orig_network_idx_order)\n",
    "\n",
    "    for i, temp_row in enumerate(new_orig_network_idx_order):\n",
    "        for j, temp_col in enumerate(new_orig_network_idx_order):\n",
    "            temp_row_ids = np.where(rows == temp_row)[0]\n",
    "            temp_col_ids = np.where(cols == temp_col)[0]\n",
    "            temp_mat = wfm_df.iloc[temp_row_ids, temp_col_ids]\n",
    "            temp_vec = temp_mat.values.ravel()\n",
    "            n_temp_vec = np.sum(np.absolute(temp_mat.values) > 1.96)\n",
    "            if mode == 'sum':\n",
    "                sum_val = temp_vec.sum()\n",
    "                n_val = n_temp_vec\n",
    "            elif mode == 'avg':\n",
    "                if temp_row == temp_col:\n",
    "                    sum_val = temp_vec.sum() / (len(temp_row_ids) * (len(temp_row_ids) - 1))\n",
    "                    n_val = n_temp_vec / (len(temp_row_ids) * (len(temp_row_ids) - 1))\n",
    "                else:\n",
    "                    sum_val = temp_vec.sum() / (len(temp_row_ids) * (len(temp_col_ids)))\n",
    "                    n_val = n_temp_vec / (len(temp_row_ids) * (len(temp_col_ids)))\n",
    "            avg_df_1.iloc[temp_row_ids, temp_col_ids] = sum_val\n",
    "            avg_df_2.loc[temp_row, temp_col] = sum_val\n",
    "            avg_df_3.loc[temp_row, temp_col] = n_val\n",
    "\n",
    "    if threshold==False:\n",
    "        avg_mat = np.array(avg_df_2.values, dtype=np.float)\n",
    "        avg_mat = (avg_mat - avg_mat.mean()) / avg_mat.std()\n",
    "        avg_mat_df = pd.DataFrame(avg_mat, columns=new_orig_network_idx_order, index=new_orig_network_idx_order)\n",
    "        avg_mat_df = avg_mat_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=0)\n",
    "        avg_mat_df = avg_mat_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=1)\n",
    "        thr_zero = 0\n",
    "        avg_mat_df[(avg_mat_df < thr_zero) & (avg_mat_df > -thr_zero)] = 0\n",
    "\n",
    "        sns.set(style=\"white\", font_scale=4.5)\n",
    "        fig, ax = plt.subplots(figsize=(32, 32))\n",
    "        threshold = np.abs(avg_mat_df.values).max()\n",
    "        threshold = 5.5\n",
    "        cbar_kws = dict(\n",
    "            use_gridspec=False, shrink=0.85, location=\"right\", label=\"Correlation ($r$)\")\n",
    "        sns.heatmap(\n",
    "            avg_mat_df, square=True, cmap=\"vlag\", ax=ax, \n",
    "            mask=np.triu(np.ones(avg_mat_df.shape), 1).astype(np.bool),\n",
    "            vmax=threshold, vmin=-threshold, \n",
    "            linewidths=5, \n",
    "            cbar=True, cbar_kws=cbar_kws,\n",
    "            fmt=\".2f\", annot=True, annot_kws={\"fontsize\": 32}\n",
    "        )\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.set_label(\"$WF$\", fontsize=60, labelpad=50)\n",
    "        ax.set_xticklabels(new_orig_network_order, rotation=90, fontsize=60)\n",
    "        ax.set_yticklabels(new_orig_network_order, rotation=0, fontsize=60)\n",
    "        plt.title(f\"Epoch {epoch}/{epochs}\")\n",
    "        plt.savefig(wfm_save_dir+f\"/{mode}_WFM_epoch{epoch}.jpg\")\n",
    "        plt.close(fig)\n",
    "    #     plt.show()\n",
    "\n",
    "    if threshold==True:\n",
    "        avg_mat = np.array(avg_df_2.values, dtype=np.float)\n",
    "        avg_mat = (avg_mat - avg_mat.mean()) / avg_mat.std()\n",
    "        avg_mat_df = pd.DataFrame(avg_mat, columns=new_orig_network_idx_order, index=new_orig_network_idx_order)\n",
    "        avg_mat_df = avg_mat_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=0)\n",
    "        avg_mat_df = avg_mat_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=1)\n",
    "\n",
    "        annot = np.vectorize(lambda x: '' if np.absolute(x) < 3.091 else str(round(x, 2)))(avg_mat_df.to_numpy())\n",
    "        # annot = np.vectorize(lambda x: '' if np.absolute(x) < 2.58 else str(round(x, 2)))(avg_mat_df.to_numpy())\n",
    "        # annot = np.vectorize(lambda x: '' if np.absolute(x) < 1.96 else str(round(x, 2)))(avg_mat_df.to_numpy())\n",
    "\n",
    "        thr_zero = 0\n",
    "        avg_mat_df[(avg_mat_df < thr_zero) & (avg_mat_df > -thr_zero)] = 0\n",
    "\n",
    "        sns.set(style=\"white\", font_scale=5)\n",
    "        plt.rcParams['mathtext.fontset'] = 'custom'\n",
    "        plt.rcParams['mathtext.it'] = 'Arial:italic'\n",
    "        plt.rcParams['mathtext.rm'] = 'Arial'\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(32, 32))\n",
    "        threshold = np.abs(avg_mat_df.values).max()\n",
    "        threshold = 7\n",
    "        cbar_kws = dict(\n",
    "            use_gridspec=False, shrink=0.85, location=\"right\", label=\"Correlation ($r$)\")\n",
    "        sns.heatmap(\n",
    "            avg_mat_df, square=True, cmap=\"vlag\",\n",
    "            ax=ax,\n",
    "            mask=np.triu(np.ones(avg_mat_df.shape), 1).astype(np.bool),\n",
    "            vmax=threshold, vmin=-threshold, \n",
    "            linewidths=5, \n",
    "            cbar=True, cbar_kws=cbar_kws,\n",
    "            fmt=\"\", annot=annot, annot_kws={\"fontsize\": 36}\n",
    "        )\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.set_label(\"$WF$\", fontsize=60, labelpad=50)\n",
    "        ax.set_xticklabels(new_orig_network_order, rotation=90, fontsize=60)\n",
    "        ax.set_yticklabels(new_orig_network_order, rotation=0, fontsize=60)\n",
    "        plt.title(f\"Epoch {epoch}/{epochs}\")\n",
    "        plt.savefig(wfm_save_dir+f\"/{mode}_WFM_epoch{epoch}.jpg\")\n",
    "        plt.close(fig)\n",
    "    #     plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_result(epoch,pearsonr,valid_prediction,valid_true):\n",
    "    \n",
    "    pred_save_dir = outer_save_dir+'/Prediction/'\n",
    "    if not os.path.isdir(pred_save_dir):\n",
    "        os.mkdir(pred_save_dir)\n",
    "    \n",
    "    sns.set(style=\"darkgrid\", font_scale=2)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    fig = sns.scatterplot(valid_true, valid_prediction,s=100)\n",
    "    plt.title(f\"Epoch {epoch}/{epochs}, Pearson's r : %.3f\" % pearsonr)\n",
    "    plt.xlabel(\"True $p$-factor\")\n",
    "    plt.ylabel(\"Predicted $p$-factor\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylim(np.min(valid_prediction)-0.0002,np.max(valid_prediction)+0.0002)\n",
    "    plt.savefig(pred_save_dir+f\"/Prediction_epoch{epoch}.png\")\n",
    "    plt.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make dataset & Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters for training\n",
    "mode = \"max\"\n",
    "min_lr = 1e-08\n",
    "lr_alpha = -1.5\n",
    "lr_beta = 1.7\n",
    "\n",
    "momentum = 0.90\n",
    "l1_param = 0\n",
    "early_stopping_patience = 150\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "n_classes = 1\n",
    "output_dim = n_classes\n",
    "\n",
    "#Hyperparameter for sparsity control\n",
    "wsc_flag = [1, 1]\n",
    "beta_lr = [5e-03, 1e-02]\n",
    "max_beta = [5e-02, 1e-01]\n",
    "n_wsc = wsc_flag.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset\n",
    "class TrainDataset(Dataset): \n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        X_train = torch.from_numpy(self.X_train[idx]).type(torch.FloatTensor)\n",
    "        y_train = torch.from_numpy(self.y_train[idx]).type(torch.FloatTensor)\n",
    "\n",
    "        return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "class TestDataset(Dataset): \n",
    "    def __init__(self, X_test, y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_test)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        X_test = torch.from_numpy(self.X_test[idx]).type(torch.FloatTensor)\n",
    "        y_test = torch.from_numpy(self.y_test[idx]).type(torch.FloatTensor)\n",
    "        \n",
    "        return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build model\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, h1, h2, dropout_h1, dropout_h2, act_func_name):\n",
    "        super(DNN, self).__init__()\n",
    "        self.ext_1 = nn.Linear(input_dim, h1)\n",
    "        self.ext_bn_1 = nn.BatchNorm1d(h1)\n",
    "        \n",
    "        self.prd_1 = nn.Linear(h1, h2)\n",
    "        self.prd_bn_1 = nn.BatchNorm1d(h2)\n",
    "        self.prd_2 = nn.Linear(h2, output_dim)\n",
    "        \n",
    "        self.dropout_h1 = nn.Dropout(p=dropout_h1)\n",
    "        self.dropout_h2 = nn.Dropout(p=dropout_h2)\n",
    "        \n",
    "        self.act_func = get_act_func(act_func_name)\n",
    "        self.weights_init()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.ext_1(x)\n",
    "        x = self.ext_bn_1(x)\n",
    "        x = self.act_func(x)\n",
    "        x = self.dropout_h1(x)\n",
    "        \n",
    "        x = self.prd_1(x)\n",
    "        x = self.prd_bn_1(x)\n",
    "        x = self.act_func(x)\n",
    "        x = self.dropout_h2(x)\n",
    "        out = self.prd_2(x)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def weights_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "                nn.init.normal_(m.bias, std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, opt_name, learning_rate=None, l2_param=None):\n",
    "    lower_opt_name = opt_name.lower()\n",
    "    if lower_opt_name == 'momentum':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                         momentum=momentum, weight_decay=l2_param)\n",
    "    elif lower_opt_name == 'nag':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                         momentum=momentum, weight_decay=l2_param, nesterov=True)\n",
    "    elif lower_opt_name == 'adam':\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate, \n",
    "                          weight_decay=l2_param)\n",
    "    elif lower_opt_name == 'sparseadam':\n",
    "        return optim.SparseAdam(model.parameters(), lr=learning_rate,\n",
    "                       betas=(0.9, 0.999), eps=1e-08, maximize=False)\n",
    "    elif lower_opt_name == 'radam':\n",
    "        return optim.RAdam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999),\n",
    "                           eps=1e-08, weight_decay=l2_param)\n",
    "    elif lower_opt_name == 'nadam':\n",
    "        return optim.NAdam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08,\n",
    "                  weight_decay=l2_param, momentum_decay=0.004)\n",
    "    elif lower_opt_name == 'adamax':\n",
    "        return optim.Adamax(model.parameters(), lr=learning_rate, betas=(0.9, 0.999),\n",
    "                            eps=1e-08, weight_decay=l2_param)\n",
    "    else:\n",
    "        sys.exit(\"Illegal arguement for optimizer type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_act_func(act_func_name):\n",
    "    act_func_name = act_func_name.lower()\n",
    "    if act_func_name == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif act_func_name == 'prelu':\n",
    "        return nn.PReLU()\n",
    "    elif act_func_name == 'elu':\n",
    "        return nn.ELU()\n",
    "    elif act_func_name == 'silu':\n",
    "        return nn.SiLU()\n",
    "    elif act_func_name == 'leakyrelu':\n",
    "        return nn.LeakyReLU()\n",
    "    elif act_func_name == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif act_func_name == 'selu':\n",
    "        return nn.SELU()\n",
    "    elif act_func_name == 'gelu':\n",
    "        return nn.GELU()\n",
    "    else:\n",
    "        sys.exit(\"Illegal arguement for activation function type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hsp(n_wsc, epochs):\n",
    "    hsp_val = torch.zeros(n_wsc)\n",
    "    beta_val = torch.clone(hsp_val)\n",
    "    hsp_list = torch.zeros((n_wsc, epochs))\n",
    "    beta_list = torch.zeros((n_wsc, epochs))\n",
    "    \n",
    "    return hsp_val, beta_val, hsp_list, beta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight sparsity control with Hoyer's sparsness (Layer wise)\n",
    "def calc_hsp(w, beta, max_beta, beta_lr, tg_hsp):\n",
    "    \n",
    "    # Get value of weight\n",
    "    [dim, n_nodes] = w.shape\n",
    "    num_elements = dim * n_nodes\n",
    "    norm_ratio = torch.norm(w.detach(), 1) / torch.norm(w.detach(), 2)\n",
    "\n",
    "    # Calculate hoyer's sparsity level\n",
    "    num = math.sqrt(num_elements) - norm_ratio\n",
    "    den = math.sqrt(num_elements) - 1\n",
    "    hsp = torch.tensor(num / den).to(device)\n",
    "\n",
    "    # Update beta\n",
    "    beta = beta.clone() + beta_lr * torch.sign(torch.tensor(tg_hsp).to(device) - hsp)\n",
    "    \n",
    "    # Trim value\n",
    "    beta = 0 if beta < 0 else beta\n",
    "    beta = max_beta if beta > max_beta else beta\n",
    "\n",
    "    return [hsp, beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_l1(model, epoch, hsp_val, beta_val, hsp_list, beta_list, tg_hsp):\n",
    "    l1_reg = None\n",
    "    layer_idx = 0\n",
    "    wsc_idx = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and \"bn\" not in name:\n",
    "            if \"ext\" in name or \"prd_1\" in name:\n",
    "                temp_w = param\n",
    "                \n",
    "                if wsc_flag[layer_idx] != 0:\n",
    "                    hsp_val[wsc_idx], beta_val[wsc_idx] = calc_hsp(\n",
    "                        temp_w, beta_val[wsc_idx], max_beta[wsc_idx], \n",
    "                        beta_lr[wsc_idx], tg_hsp[wsc_idx]\n",
    "                    )\n",
    "                    hsp_list[wsc_idx, epoch - 1] = hsp_val[wsc_idx]\n",
    "                    beta_list[wsc_idx, epoch - 1] = beta_val[wsc_idx]\n",
    "                    layer_reg = torch.norm(temp_w, 1) * beta_val[wsc_idx].clone()\n",
    "                    wsc_idx += 1\n",
    "                else:\n",
    "                    layer_reg = torch.norm(temp_w, 1) * l1_param\n",
    "\n",
    "                if l1_reg is None:\n",
    "                    l1_reg = layer_reg\n",
    "                else:\n",
    "                    l1_reg = l1_reg + layer_reg\n",
    "                layer_idx += 1\n",
    "        \n",
    "    return l1_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pearsonr(x, y):\n",
    "    x_mean = torch.mean(x.detach())\n",
    "    y_mean = torch.mean(y.detach())\n",
    "    xx = x.sub(x_mean)\n",
    "    yy = y.sub(y_mean)\n",
    "    num = xx.dot(yy)\n",
    "    den = torch.norm(xx, 2) * torch.norm(yy, 2)\n",
    "    corr = num / den\n",
    "    return corr.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, train_loader, optimizer, criterion, \n",
    "          hsp_val, beta_val, hsp_list, beta_list, tg_hsp):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    cost = 0\n",
    "    total = 0\n",
    "    y_train_true = []\n",
    "    y_train_pred = []\n",
    "    \n",
    "    \n",
    "    for batch_idx, (input, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        pred, target = model(input), target.view(-1, 1)\n",
    "        l1_norm = calc_l1(model, epoch, hsp_val, beta_val, hsp_list, beta_list, tg_hsp)\n",
    "        running_loss = criterion(pred, target)\n",
    "        cost = running_loss + l1_norm.clone()\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += running_loss.item()\n",
    "        total += pred.size(0)\n",
    "        \n",
    "        true_batch = torch.flatten(target).detach().cpu().numpy()\n",
    "        pred_batch = torch.flatten(pred).detach().cpu().numpy()\n",
    "        y_train_true.extend(true_batch)\n",
    "        y_train_pred.extend(pred_batch)\n",
    "    \n",
    "    train_loss = train_loss / (batch_idx+1)\n",
    "    train_acc,train_p = stats.pearsonr(y_train_true, y_train_pred)\n",
    "    train_mae = mean_absolute_error(y_train_true, y_train_pred)\n",
    "\n",
    "    return train_loss, train_acc,train_p,train_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    test_acc = 0\n",
    "    total = 0\n",
    "    y_test_true = []\n",
    "    y_test_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, target) in enumerate(test_loader):\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            pred, target = model(input), target.view(-1, 1)\n",
    "            running_loss = criterion(pred, target)\n",
    "            test_loss += running_loss.item()\n",
    "            total += pred.size(0)\n",
    "\n",
    "            true_batch = torch.flatten(target).detach().cpu().numpy()\n",
    "            pred_batch = torch.flatten(pred).detach().cpu().numpy()\n",
    "            y_test_true.extend(true_batch)\n",
    "            y_test_pred.extend(pred_batch)\n",
    "\n",
    "        test_acc,test_p = stats.pearsonr(y_test_true, y_test_pred)\n",
    "        plot_prediction_result(epoch,test_acc,y_test_pred,y_test_true)\n",
    "        test_loss = test_loss / (batch_idx+1)\n",
    "        test_mae = mean_absolute_error(y_test_true, y_test_pred)\n",
    "\n",
    "\n",
    "    return test_loss, test_acc, test_p,test_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(\n",
    "    save_dir, epochs, train_loss, valid_loss, train_acc, valid_acc, \n",
    "    lr, plot_hsp_list, plot_beta_list, tg_hsp, val_flg):\n",
    "    \n",
    "    sns.set(style=\"darkgrid\", font_scale=2)\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(20, 10))\n",
    "    ax = ax.flat\n",
    "    lw = 4\n",
    "    last_epoch = epochs\n",
    "    \n",
    "    ax[0].plot(train_loss[:last_epoch], label='train loss', lw=lw, color=\"g\")\n",
    "    if val_flg:\n",
    "        ax[0].plot(valid_loss[:last_epoch], label='valid loss', lw=lw, color=\"orange\")\n",
    "    else:\n",
    "        ax[0].plot(valid_loss[:last_epoch], label='test loss', lw=lw, color=\"r\")\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(\"Loss Plot\", pad=10)\n",
    "\n",
    "    ax[1].plot(train_acc[:last_epoch], label='train corr', lw=lw, color=\"g\")\n",
    "    if val_flg:\n",
    "        ax[1].plot(valid_acc[:last_epoch], label='valid corr', lw=lw, color=\"orange\")\n",
    "    else:\n",
    "        ax[1].plot(valid_acc[:last_epoch], label='test corr', lw=lw, color=\"r\")\n",
    "    ax[1].legend()\n",
    "    ax[1].set_title(\"Correlation Plot\", pad=10)\n",
    "\n",
    "    plot_hsp_list, plot_beta_list = np.array(plot_hsp_list).T, np.array(plot_beta_list).T\n",
    "    \n",
    "    for idx, n_layer in enumerate(indices):\n",
    "        ax[3].plot(plot_hsp_list[idx], label='layer{}'.format(n_layer), lw=lw)\n",
    "        ax[4].plot(plot_beta_list[idx], \n",
    "                   label='layer{}'.format(n_layer), lw=lw)\n",
    "        ax[3].legend(); ax[4].legend()\n",
    "        ax[3].set_title(\"HSP plot [{:.3f}/{:.3f}]\"\n",
    "                        .format(plot_hsp_list[0, -1], tg_hsp[0][0]), pad=10)\n",
    "        ax[4].set_title(\"Beta plot\", pad=10)\n",
    "    \n",
    "    #Learning rate plot\n",
    "    ax[5].plot(lr[:last_epoch],label='Learning rate',lw=lw,color='m')\n",
    "    ax[5].set_title(\"Learning rate\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"{}/Learning_curves.png\".format(save_dir))\n",
    "    \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_outer_fold(n_outer_cv=0, outer_save_dir=None, sel_tg_hsp=None):\n",
    "    val_flg = 0\n",
    "    outer_cv_list = []\n",
    "    \n",
    "    # Outer fold\n",
    "    print(\"\\n===================================\", end=\" \")\n",
    "    print(\"Outer Fold [{}/{}]\".format(n_outer_cv + 1, outer_n_splits), end=\" \")\n",
    "    print(\"===================================\")\n",
    "    \n",
    "    outer_start_fold_time = time.time()\n",
    "    outer_train_idx = outer_train_folds_idx[n_outer_cv]\n",
    "    outer_test_idx = outer_test_folds_idx[n_outer_cv]\n",
    "\n",
    "    X_train, y_train = X[outer_train_idx], y[outer_train_idx]\n",
    "    X_test, y_test = X[outer_test_idx], y[outer_test_idx]\n",
    "    \n",
    "    X_train = stats.zscore(X_train, axis=1)\n",
    "    X_test = stats.zscore(X_test, axis=1)\n",
    "        \n",
    "    outer_train_dataset = TrainDataset(X_train, y_train)\n",
    "    outer_test_dataset = TestDataset(X_test, y_test)\n",
    "    \n",
    "    outer_train_loader = DataLoader(\n",
    "        outer_train_dataset, batch_size=batch_size, pin_memory=True,\n",
    "        shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "    outer_test_loader = DataLoader(\n",
    "        outer_test_dataset, batch_size=len(y_test), pin_memory=True,\n",
    "        shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "        \n",
    "    # Assign model \n",
    "    model = DNN(h1, h2, dropout_h1, dropout_h2, act_func_name).to(device)\n",
    "    if pretrain:\n",
    "        model_dict = model.state_dict()\n",
    "        model_dict.update(pretrained_model_dict)\n",
    "        model.load_state_dict(pretrained_model_dict)\n",
    "\n",
    "        for name, child in model.named_children():\n",
    "            if \"ext\" in name:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = freeze_ext\n",
    "            elif \"prd_1\" in name:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = freeze_prd\n",
    "    else:\n",
    "        pass\n",
    "            \n",
    "    visualize_wfm(model,1,epoch_gap = 1,threshold=True, mode= 'sum')\n",
    "    visualize_wfm(model,1,epoch_gap = 1,threshold=True, mode= 'avg')\n",
    "    \n",
    "    \n",
    "    optimizer = get_optimizer(model, optimizer_name, learning_rate, l2_param)\n",
    "    lr_factor = lr_alpha * sel_tg_hsp[0][0] + lr_beta\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=mode, patience=lr_patience, min_lr=min_lr, factor=lr_factor\n",
    "    )\n",
    "    cosine_scheduler = CosineAnnealingWarmRestarts(optimizer,25,eta_min = min_lr)\n",
    "    criterion = nn.MSELoss()\n",
    "              \n",
    "    # list to save learning parameters\n",
    "    outer_train_loss = []\n",
    "    outer_test_loss = []\n",
    "    outer_train_acc = []\n",
    "    outer_test_acc = []\n",
    "    outer_lr = []\n",
    "    outer_hsp_list = []\n",
    "    outer_beta_list = []\n",
    "\n",
    "    hsp_val, beta_val, hsp_list, beta_list = init_hsp(n_wsc, epochs)\n",
    "    \n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss, train_acc,train_p, train_mae = train(\n",
    "            model, epoch, outer_train_loader, optimizer, criterion, \n",
    "            hsp_val, beta_val, hsp_list, beta_list, sel_tg_hsp\n",
    "        )\n",
    "        test_loss, test_acc,test_p, test_mae = test(model, epoch, outer_test_loader, criterion)\n",
    "        if freeze_ext and hsp_val[0] < temp_param['hsp_h1']:\n",
    "            scheduler.step(hsp_val[0])\n",
    "        if hsp_val[0] >= temp_param['hsp_h1'] and epoch>100:\n",
    "            cosine_scheduler.step()\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        outer_train_loss.append(train_loss)\n",
    "        outer_train_acc.append(train_acc)\n",
    "        outer_test_loss.append(test_loss)\n",
    "        outer_test_acc.append(test_acc)\n",
    "        outer_lr.append(lr)\n",
    "        outer_hsp_list.append(hsp_val.tolist())\n",
    "        outer_beta_list.append(beta_val.tolist())\n",
    "\n",
    "        if epoch % print_epoch == 0:\n",
    "            print(\"\\nEpoch [{:d}/{:d}]\".format(epoch, epochs), end=\" \")\n",
    "            print(\"Train acc: {:.2f}, Test acc: {:.2f}, Train loss: {:.4f}, Test loss: {:.4f}, Current_lr: {:.8f}\"\n",
    "                  .format(train_acc, test_acc, train_loss, test_loss,lr))\n",
    "            for i in range(len(wsc_flag)):\n",
    "                if wsc_flag[i] != 0:\n",
    "                    print(\"Layer {:d}: [{:.4f}/{:.4f}]\".\n",
    "                          format( i + 1, hsp_val[i], sel_tg_hsp[i][0]), end=\" \")\n",
    "\n",
    "            plot_learning_curves(\n",
    "                outer_save_dir, epochs, outer_train_loss, outer_test_loss,  \n",
    "                outer_train_acc, outer_test_acc, \n",
    "                outer_lr, outer_hsp_list, outer_beta_list, sel_tg_hsp, val_flg\n",
    "            )\n",
    "\n",
    "        \n",
    "    torch.save(model.state_dict(), \n",
    "               outer_save_dir + \"/model_fold_\" + str(n_outer_cv + 1) + \".pt\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    outer_cv_list.append([train_acc, test_acc,train_p,test_p,test_loss,train_mae,test_mae])\n",
    "            \n",
    "    outer_cv_df = pd.DataFrame(np.array(outer_cv_list), columns=[\"train_acc\", \"test_acc\",\"train_corr_p\",\n",
    "                                                                 \"test_corr_p\",\"test_loss\",\"train_mae\",\n",
    "                                                                 \"test_mae\"])\n",
    "    outer_cv_df.to_csv(\"{}/outer_cv.csv\".format(outer_save_dir))\n",
    "\n",
    "    outer_tot_time = time.time() - outer_start_fold_time\n",
    "    print(\"\\nExecution Time for Fold: {:.2f} mins\".format(outer_tot_time / 60))\n",
    "    \n",
    "    return train_acc, test_acc, outer_hsp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Settle parameters, output path & Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = list(ParameterGrid(param_cand))\n",
    "\n",
    "temp_param = param_grid[temp_sel_idx]\n",
    "\n",
    "h1 = temp_param[\"h1\"]\n",
    "h2 = temp_param[\"h2\"]\n",
    "\n",
    "dropout_h1 = temp_param[\"dropout_h1\"]\n",
    "dropout_h2 = temp_param[\"dropout_h2\"]\n",
    "\n",
    "pretrain = temp_param[\"pretrain\"]\n",
    "freeze_ext = temp_param[\"freeze_ext\"]\n",
    "freeze_prd = temp_param[\"freeze_prd\"]\n",
    "\n",
    "batch_size = temp_param[\"batch_size\"]\n",
    "learning_rate = temp_param[\"lr\"]\n",
    "epochs = temp_param[\"epochs\"]\n",
    "l2_param = temp_param[\"l2_param\"]\n",
    "\n",
    "lr_patience = temp_param[\"lr_patience\"]\n",
    "lr_factor = temp_param[\"lr_factor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNN(h1, h2, dropout_h1, dropout_h2, act_func_name).to(device)\n",
    "cv_dir = prt_dir\n",
    "cv_list = [i for i in os.listdir(cv_dir) if \"Outer_fold_Full\" not in i and 'WFM' not in i and \"_weights\" not in i]\n",
    "cv_list.sort()\n",
    "\n",
    "model_list = []\n",
    "new_k = []\n",
    "new_v = []\n",
    "\n",
    "for i, j in enumerate(cv_list):\n",
    "    tmp_path = \"{}/model_fold_{}.pt\".format(j, i + 1)\n",
    "    model_path = os.path.join(cv_dir, tmp_path)\n",
    "    tmp_model = torch.load(model_path)\n",
    "    v_list = []\n",
    "    for k, v in tmp_model.items():\n",
    "        if k in model.state_dict():\n",
    "            v_list.append(v)\n",
    "            if i == 0:\n",
    "                new_k.append(k)\n",
    "    model_list.append(v_list)\n",
    "\n",
    "for j in range(len(v_list)):\n",
    "    for l, v in enumerate(model_list):\n",
    "        if l == 0:\n",
    "            tmp = model_list[l][j]\n",
    "        else:\n",
    "            tmp = torch.add(tmp, model_list[l][j])\n",
    "    tmp_v = tmp / len(model_list)\n",
    "    new_v.append(tmp_v)\n",
    "    \n",
    "pretrained_model_dict = {new_k[i]: new_v[i] for i in range(len(v_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/hjd/IG_my_study/SNUH/data/temp/trasfer/p/Hsp:[0.95,0.3]_Maxb:[0.05, 0.1]_Betalr:[0.005, 0.01]_LR:[1e-05]_L2:[0.2]_Act:[elu]_Opt:[nag]_DO:[0.3,0.3]_epoch:[50]_pretrain:True_trainext:False_trainprd:True_seed:19_from[0.95,0.3,0.3]\n"
     ]
    }
   ],
   "source": [
    "prt_hsp = prt_dir.split(\"Hsp:\")[1].split(\"_\")[0]\n",
    "save_folder = f\"Hsp:[{temp_param['hsp_h1']*wsc_flag[0]},{temp_param['hsp_h2']*wsc_flag[1]}]_Maxb:{max_beta}_Betalr:{beta_lr}_LR:[{temp_param['lr']}]_L2:[{l2_param}]_Act:[{act_func_name}]_Opt:[{optimizer_name}]_DO:[{temp_param['dropout_h1']},{temp_param['dropout_h2']}]_epoch:[{epochs}]_pretrain:{pretrain}_trainext:{freeze_ext}_trainprd:{freeze_prd}_seed:{seed}_from{prt_hsp}\"\n",
    "save_path = f\"/users/hjd/IG_my_study/SNUH/data/temp/trasfer/{target_name}\"\n",
    "output_folder = os.path.join(save_path, save_folder)\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    \n",
    "elif len(os.listdir(output_folder))>0:\n",
    "    for fold in [f'Outer_fold_{i+1}' for i in select_fold]:\n",
    "        if fold in os.listdir(output_folder):\n",
    "            print(output_folder)\n",
    "            raise Exception(\"Result already exist. Check directory!\")\n",
    "            \n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/hjd/IG_my_study/SNUH/data/temp/trasfer/p/Hsp:[0.95,0.3]_Maxb:[0.05, 0.1]_Betalr:[0.005, 0.01]_LR:[1e-05]_L2:[0.2]_Act:[elu]_Opt:[nag]_DO:[0.3,0.3]_epoch:[50]_pretrain:True_trainext:False_trainprd:True_seed:19_from[0.95,0.3,0.3]\n",
      "\n",
      "=================================== Outer Fold [1/20] ===================================\n",
      "Selected param: hsp_h1: 0.95 hsp_h2: 0.3 \n",
      "=================================== Outer Fold [1/20] ===================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "findfont: Font family ['cursive'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n",
      "findfont: Font family ['Arial'] not found. Falling back to DejaVu Sans.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch [5/50] Train acc: 0.43, Test acc: 0.03, Train loss: 0.4628, Test loss: 0.4588, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3026/0.3000] \n",
      "Epoch [10/50] Train acc: 0.33, Test acc: -0.06, Train loss: 0.4393, Test loss: 0.4708, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3025/0.3000] \n",
      "Epoch [15/50] Train acc: 0.48, Test acc: -0.14, Train loss: 0.3868, Test loss: 0.4878, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3024/0.3000] \n",
      "Epoch [20/50] Train acc: 0.53, Test acc: -0.08, Train loss: 0.3728, Test loss: 0.4803, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3023/0.3000] \n",
      "Epoch [25/50] Train acc: 0.57, Test acc: -0.22, Train loss: 0.3489, Test loss: 0.5226, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3023/0.3000] \n",
      "Epoch [30/50] Train acc: 0.49, Test acc: -0.15, Train loss: 0.3742, Test loss: 0.5031, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3022/0.3000] \n",
      "Epoch [35/50] Train acc: 0.59, Test acc: -0.14, Train loss: 0.3280, Test loss: 0.5057, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3021/0.3000] \n",
      "Epoch [40/50] Train acc: 0.56, Test acc: -0.15, Train loss: 0.3421, Test loss: 0.5217, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3020/0.3000] \n",
      "Epoch [45/50] Train acc: 0.60, Test acc: -0.21, Train loss: 0.3207, Test loss: 0.5381, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3020/0.3000] \n",
      "Epoch [50/50] Train acc: 0.56, Test acc: -0.16, Train loss: 0.3357, Test loss: 0.5369, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3019/0.3000] \n",
      "Execution Time for Fold: 3.42 mins\n",
      "\n",
      "Outer Fold [1/20]: train acc: 0.5615, test acc: -0.1589\n",
      "\n",
      "=================================== Outer Fold [2/20] ===================================\n",
      "Selected param: hsp_h1: 0.95 hsp_h2: 0.3 \n",
      "=================================== Outer Fold [2/20] ===================================\n",
      "\n",
      "Epoch [5/50] Train acc: 0.25, Test acc: 0.00, Train loss: 0.4558, Test loss: 0.8986, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3026/0.3000] \n",
      "Epoch [10/50] Train acc: 0.49, Test acc: 0.15, Train loss: 0.3736, Test loss: 0.8292, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3025/0.3000] \n",
      "Epoch [15/50] Train acc: 0.54, Test acc: 0.09, Train loss: 0.3588, Test loss: 0.7912, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3024/0.3000] \n",
      "Epoch [20/50] Train acc: 0.47, Test acc: 0.03, Train loss: 0.3655, Test loss: 0.7938, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3024/0.3000] \n",
      "Epoch [25/50] Train acc: 0.59, Test acc: -0.09, Train loss: 0.3185, Test loss: 0.7389, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3023/0.3000] \n",
      "Epoch [30/50] Train acc: 0.61, Test acc: -0.03, Train loss: 0.3054, Test loss: 0.7585, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3023/0.3000] \n",
      "Epoch [35/50] Train acc: 0.54, Test acc: -0.04, Train loss: 0.3367, Test loss: 0.7958, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3022/0.3000] \n",
      "Epoch [40/50] Train acc: 0.47, Test acc: -0.01, Train loss: 0.3603, Test loss: 0.7501, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3022/0.3000] \n",
      "Epoch [45/50] Train acc: 0.65, Test acc: 0.03, Train loss: 0.2720, Test loss: 0.8049, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3021/0.3000] \n",
      "Epoch [50/50] Train acc: 0.59, Test acc: -0.03, Train loss: 0.3059, Test loss: 0.8054, Current_lr: 0.00001000\n",
      "Layer 1: [0.9599/0.9500] Layer 2: [0.3020/0.3000] \n",
      "Execution Time for Fold: 2.74 mins\n",
      "\n",
      "Outer Fold [2/20]: train acc: 0.5932, test acc: -0.0347\n"
     ]
    }
   ],
   "source": [
    "code_start_time = time.time()\n",
    "\n",
    "print(output_folder)\n",
    "\n",
    "outer_cv = []\n",
    "\n",
    "for n_outer_cv in outer_cv_part:\n",
    "    if n_outer_cv not in select_fold:\n",
    "        continue    \n",
    "    print(\"\\n===================================\", end=\" \")\n",
    "    print(\"Outer Fold [{}/{}]\".format(n_outer_cv + 1, outer_n_splits), end=\" \")\n",
    "    print(\"===================================\")\n",
    "\n",
    "    outer_save_dir = \"{}/Outer_fold_{}\".format(output_folder, n_outer_cv + 1)\n",
    "    os.makedirs(outer_save_dir, exist_ok=True)\n",
    "\n",
    "    inner_cv = []\n",
    "    temp_inner_cv_acc = []\n",
    "    sel_idx = temp_sel_idx\n",
    "    sel_param = param_grid[sel_idx]\n",
    "    sel_hsp = []\n",
    "    print(\"Selected param:\", end=\" \")\n",
    "    for x in sel_param:\n",
    "        if \"hsp\" in x: \n",
    "            print(\"{}: {}\".format(x, sel_param[x]), end=\" \")\n",
    "            sel_hsp.append(sel_param[x])\n",
    "    \n",
    "    # Outer Fold\n",
    "    hsp_cand_1 = [sel_param[\"hsp_h1\"]]\n",
    "    hsp_cand_2 = [sel_param[\"hsp_h2\"]]\n",
    "\n",
    "    indices = [i + 1 for i, x in enumerate(wsc_flag) if x == 1]\n",
    "    hsp_cand_list = list(itertools.product(hsp_cand_1, hsp_cand_2))\n",
    "    hsp_cand_list = [list(i) for i in hsp_cand_list]\n",
    "    hsp_cand = [hsp_cand_1, hsp_cand_2]\n",
    "    sel_tg_hsp = hsp_cand\n",
    "\n",
    "    outer_train_acc, outer_test_acc, outer_hsp_list = run_outer_fold(\n",
    "        n_outer_cv, outer_save_dir, sel_tg_hsp\n",
    "    )\n",
    "    outer_cv.append([sel_hsp, outer_train_acc, outer_test_acc])\n",
    "    \n",
    "    print(\"\\nOuter Fold [{}/{}]: train acc: {:.4f}, test acc: {:.4f}\"\n",
    "          .format(n_outer_cv + 1, outer_n_splits, outer_train_acc, outer_test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time for the training: 0.10 hours\n"
     ]
    }
   ],
   "source": [
    "code_tot_time = time.time() - code_start_time \n",
    "print(\"Execution Time for the training: {:.2f} hours\".format(code_tot_time / 60 / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.5774, Test: -0.0968\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(outer_cv, columns=[\"hsp\", \"train\", \"test\"])\n",
    "train_avg = np.array([x for x in df[\"train\"].values]).mean()\n",
    "test_avg = np.array([x for x in df[\"test\"].values]).mean()\n",
    "print(\"Train: {:.4f}, Test: {:.4f}\".format(train_avg, test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 2\n",
      "dropout_h1 0.3\n",
      "dropout_h2 0.3\n",
      "epochs 50\n",
      "freeze_ext False\n",
      "freeze_prd True\n",
      "h1 1024\n",
      "h2 1024\n",
      "hsp_h1 0.95\n",
      "hsp_h2 0.3\n",
      "l2_param 0.2\n",
      "lr 1e-05\n",
      "lr_factor 0.5\n",
      "lr_patience 5\n",
      "pretrain True\n"
     ]
    }
   ],
   "source": [
    "tmp_param_grid = list(ParameterGrid(param_cand))[temp_sel_idx]\n",
    "for name in tmp_param_grid:\n",
    "    print(name, tmp_param_grid[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lr'] = learning_rate\n",
    "df['beta_lr'] = [beta_lr]*len(df)\n",
    "df['max_beta'] = [max_beta]*len(df)\n",
    "df['train_idx'] = [outer_train_folds_idx[i] for i in select_fold]\n",
    "df['test_idx'] = [outer_test_folds_idx[i] for i in select_fold]\n",
    "df['l2_param'] = l2_param\n",
    "df['batch_size'] = batch_size\n",
    "df['act_func'] = act_func_name\n",
    "df['optimizer'] = optimizer_name\n",
    "df['momentum'] = momentum\n",
    "df['epoch'] = epochs\n",
    "df['pretrain_model_path'] = prt_dir\n",
    "\n",
    "df.to_csv(output_folder+f\"/result_df_{np.array(select_fold)+1}.csv\", sep='\\t',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
