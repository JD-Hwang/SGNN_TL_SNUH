{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is code for pre-training SGNN with ABCD data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GPU number\n",
    "GPU = 0\n",
    "\n",
    "num_workers = 4\n",
    "\n",
    "#Random Seed\n",
    "seed = 42 \n",
    "\n",
    "#Target set to 'p' since we're predicting the p-factor. \n",
    "target_name = 'p' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.24.1', '1.10.1+cu102')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn, torch\n",
    "sklearn.__version__, torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index for hyperparameter selection.\n",
    "#Hyperparameter sets were selected with Paramgrid, then one specific hyperparameter was chosen based on this index.\n",
    "temp_sel_idx = 0 \n",
    "\n",
    "#Number of cross-validation. 5 as default\n",
    "n_cv = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Fold: [0 1 2 3 4]\n",
      "Selected Fold: [0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "outer_cv_part = np.arange(0, n_cv)\n",
    "print(\"Selected Fold: {}\".format(outer_cv_part))\n",
    "\n",
    "select_fold = [0]\n",
    "print(\"Selected Fold: {}\".format(select_fold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "#Number of nodes in SGNN. 1024 as default\n",
    "ext_cand = [1024]\n",
    "prd_cand = [1024]\n",
    "dsc_cand = [1024]\n",
    "\n",
    "#Dropout rate of SGNN\n",
    "dropout_ext_cand = [0.9]\n",
    "dropout_prd_cand = [0.9]\n",
    "dropout_dsc_cand = [0.0]\n",
    "\n",
    "#Activation function / Optimizer of SGNN\n",
    "act_func_name = \"elu\"\n",
    "optimizer_name = \"nag\"\n",
    "\n",
    "#Batch size / Learning rate / LR patience for scheduling / LR scaling ratio / Epochs / Freezing epochs\n",
    "batch_size_cand = [32]\n",
    "lr_cand = [5e-05]\n",
    "lr_patience_cand = [5]\n",
    "lr_factor_cand = [0.5]\n",
    "epochs_cand = [10]\n",
    "pretrain_epoch_cand = [20]\n",
    "\n",
    "#Hoyer's sparsity candidates\n",
    "hsp_ext_cand = [0.95] #[0.975, 0.95, 0.9, 0.8]\n",
    "hsp_prd_cand = [0.3]\n",
    "hsp_dsc_cand = [0.3]\n",
    "\n",
    "#Lambda for gradient reversal layer\n",
    "lambda_cand = [0.01]\n",
    "\n",
    "#L2 norm\n",
    "l2_param_cand = [5e-02]\n",
    "\n",
    "\n",
    "param_cand = {\n",
    "    \"ext\": ext_cand, \"prd\": prd_cand,\"dsc\":dsc_cand,  \n",
    "    \"dropout_ext\": dropout_ext_cand, \"dropout_prd\": dropout_prd_cand,\n",
    "    \"dropout_dsc\": dropout_dsc_cand,\"batch_size\": batch_size_cand,\n",
    "    \"lr\": lr_cand, \"epochs\": epochs_cand, \"pretrain_epoch\": pretrain_epoch_cand,\n",
    "    \"lr_patience\": lr_patience_cand, \"lr_factor\": lr_factor_cand,\n",
    "    \"hsp_ext\": hsp_ext_cand, \"hsp_prd\": hsp_prd_cand,\"hsp_dsc\": hsp_dsc_cand,\n",
    "    \"l2_param\": l2_param_cand,\"lambda_\": lambda_cand\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import imageio\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from decimal import Decimal\n",
    "from datetime import datetime as dt\n",
    "from pytz import timezone\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,QuantileTransformer,RobustScaler,PowerTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable, Function\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nowtime = dt.now(timezone(\"Asia/Seoul\")); year = str(nowtime.year)[2:]\n",
    "month = '0{}'.format(nowtime.month) if nowtime.month < 10 else str(nowtime.month)\n",
    "day = '0{}'.format(nowtime.day) if nowtime.day < 10 else str(nowtime.day)\n",
    "hour = '0{}'.format(nowtime.hour) if nowtime.hour < 10 else str(nowtime.hour)\n",
    "minute = '0{}'.format(nowtime.minute) if nowtime.minute < 10 else str(nowtime.minute)\n",
    "sec = '0{}'.format(nowtime.second) if nowtime.second < 10 else str(nowtime.second)\n",
    "msec = str(nowtime.microsecond)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Input / Target setting & Fold division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.254328727722168 -7.254328727722168\n",
      "(6905, 61776) (6905, 2)\n"
     ]
    }
   ],
   "source": [
    "#Input data: vectorized RSFC\n",
    "data_path = \"/users/hjw/data/ABCD/npz_files/rsfc_p_site_scanner_si_ge.npz\"\n",
    "data = np.load(data_path, allow_pickle=True)\n",
    "X = data[\"X\"]\n",
    "\n",
    "#Target data: factor score / scanner type\n",
    "targets_all = np.load(\"/data4/SNU/data/ABCD_CFA_5factor.npz\",allow_pickle=True) \n",
    "target_fs = targets_all[target_name]\n",
    "target_scn = targets_all['scn']\n",
    "y = np.hstack([target_fs.reshape(-1,1),target_scn.reshape(-1,1)])\n",
    "print(X.max(),X.min())\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_wfm = make_wfm(stats.zscore(X.mean(0)))\n",
    "# sns.set(style=\"white\", font_scale=1.5)\n",
    "# plt.figure(figsize=(20,15))\n",
    "# sns.heatmap(X_wfm,vmax=1.96,vmin=-1.96,cmap=\"RdBu_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_factor_idx = 0\n",
    "scanner_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5524, 690, 691)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(seed)\n",
    "\n",
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "\n",
    "outer_n_splits = n_cv\n",
    "\n",
    "outer_train_folds_idx = []\n",
    "outer_val_folds_idx = []\n",
    "outer_test_folds_idx = []\n",
    "\n",
    "outer_skf = ShuffleSplit(\n",
    "    n_splits=outer_n_splits, test_size=0.20, random_state=seed)\n",
    "\n",
    "\n",
    "if 'Full' in select_fold:\n",
    "    n_cv = 1\n",
    "    outer_train_folds_idx.append(np.arange(len(X)))\n",
    "    outer_val_folds_idx.append(np.array([0,1]))\n",
    "    outer_test_folds_idx.append(np.array([0,1]))\n",
    "\n",
    "else:\n",
    "    for n_outer, (outer_train_idx, outer_test1_idx) in enumerate(outer_skf.split(X, y)):\n",
    "        outer_train_folds_idx.append(outer_train_idx)\n",
    "#         outer_test_folds_idx.append(outer_test_idx)\n",
    "        outer_val_idx = np.random.choice(outer_test1_idx,size=len(outer_test1_idx)//2,replace=False)\n",
    "        outer_test_idx = np.array([i for i in outer_test1_idx if i not in outer_val_idx])\n",
    "        outer_val_folds_idx.append(outer_val_idx)\n",
    "        outer_test_folds_idx.append(outer_test_idx)\n",
    "\n",
    "len(outer_train_folds_idx),len(outer_train_folds_idx[0]), len(outer_val_folds_idx[0]), len(outer_test_folds_idx[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Real-time visualization code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(352,)\n",
      "['AUD', 'CON', 'CPAR', 'DAN', 'DMN', 'FPN', 'RSP', 'SAL', 'SCN', 'SMH', 'SMM', 'VAN', 'VIS', 'NONE']\n",
      "(352, 352)\n"
     ]
    }
   ],
   "source": [
    "num_ROIs = tot_rois = 352\n",
    "threshold = 1.96\n",
    "\n",
    "# Preparing draw feature map\n",
    "parcels = pd.read_excel(\"/users/hjw/data/ABCD/Parcels/Parcels.xlsx\", engine=\"openpyxl\")\n",
    "networks = list(parcels[\"Community\"]) + 19 * [\"Subcortex\"]\n",
    "\n",
    "networks_df = pd.DataFrame(networks, columns=[\"network\"])\n",
    "networks_df[networks_df[\"network\"] == \"Auditory\"] = \"AUD\"\n",
    "networks_df[networks_df[\"network\"] == \"Visual\"] = \"VIS\"\n",
    "networks_df[networks_df[\"network\"] == \"VentralAttn\"] = \"VAN\"\n",
    "networks_df[networks_df[\"network\"] == \"Subcortex\"] = \"SCN\"\n",
    "networks_df[networks_df[\"network\"] == \"Salience\"] = \"SAL\"\n",
    "networks_df[networks_df[\"network\"] == \"SMmouth\"] = \"SMM\"\n",
    "networks_df[networks_df[\"network\"] == \"SMhand\"] = \"SMH\"\n",
    "networks_df[networks_df[\"network\"] == \"RetrosplenialTemporal\"] = \"RSP\"\n",
    "networks_df[networks_df[\"network\"] == \"None\"] = \"NONE\"\n",
    "networks_df[networks_df[\"network\"] == \"FrontoParietal\"] = \"FPN\"\n",
    "networks_df[networks_df[\"network\"] == \"DorsalAttn\"] = \"DAN\"\n",
    "networks_df[networks_df[\"network\"] == \"Default\"] = \"DMN\"\n",
    "networks_df[networks_df[\"network\"] == \"CinguloParietal\"] = \"CPAR\"\n",
    "networks_df[networks_df[\"network\"] == \"CinguloOperc\"] = \"CON\"\n",
    "\n",
    "networks = np.array(networks_df[\"network\"]).astype(\"str\")\n",
    "network_label = np.unique(networks, return_index=True)[0].astype(\"str\")\n",
    "orig_network_path = \"/users/hjw/data/ABCD/npz_files/Gordon_network_labels.npz\"\n",
    "orig_networks = np.load(orig_network_path)[\"networks\"]\n",
    "print(orig_networks.shape)\n",
    "\n",
    "# Set order of network label \n",
    "new_orig_network_idx_order = [\n",
    "    'AUD', 'DAN', 'CPAR', 'NONE', \n",
    "    'SAL', 'VIS', 'RSP', 'DMN',\n",
    "    'SMM', 'CON', 'SCN', \n",
    "    'SMH', 'VAN', 'FPN'\n",
    "]\n",
    "\n",
    "sorted_order = sorted(new_orig_network_idx_order)\n",
    "sorted_order.remove(\"NONE\")\n",
    "sorted_order.append(\"NONE\")\n",
    "new_orig_network_idx_order = sorted_order\n",
    "print(new_orig_network_idx_order)\n",
    "\n",
    "new_orig_network_order = {\n",
    "    key:value for (key, value) in \n",
    "    zip(new_orig_network_idx_order, (np.arange(len(new_orig_network_idx_order))))\n",
    "}\n",
    "\n",
    "\n",
    "ref_orig_wfm = np.zeros((tot_rois, tot_rois))\n",
    "print(ref_orig_wfm.shape)\n",
    "ref_orig_wfm = pd.DataFrame(ref_orig_wfm, index=orig_networks, columns=orig_networks)\n",
    "ref_orig_wfm = ref_orig_wfm.sort_index(key=lambda x: x.map(new_orig_network_order), \n",
    "                                       axis=0)\n",
    "ref_orig_wfm = ref_orig_wfm.sort_index(key=lambda x: x.map(new_orig_network_order), \n",
    "                                       axis=1)    \n",
    "\n",
    "network_unq = new_orig_network_idx_order\n",
    "sorted_networks = np.array(ref_orig_wfm.columns, dtype=np.str)\n",
    "\n",
    "\n",
    "sorted_networks_df = pd.DataFrame(np.unique(sorted_networks, return_index=True)).T\n",
    "sorted_networks_df.columns = [\"networks\", \"n\"]\n",
    "sorted_networks_df = sorted_networks_df.sort_values(\n",
    "    by=\"networks\", key=lambda x: x.map(new_orig_network_order)\n",
    ")\n",
    "\n",
    "start_network_idx = np.array(sorted_networks_df.n)\n",
    "next_network_idx = np.hstack((start_network_idx[1:], 352))\n",
    "\n",
    "network_mid_idx = np.array((start_network_idx + next_network_idx) / 2, dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_wfm(vec):\n",
    "    wfm = np.zeros((tot_rois, tot_rois))\n",
    "    iu_non_di_idx = np.mask_indices(tot_rois, np.triu, 1)\n",
    "    wfm[iu_non_di_idx] = vec\n",
    "    il_idx = np.tril_indices(tot_rois, -1)\n",
    "    wfm[il_idx] = wfm.T[il_idx]\n",
    "    wfm_df = pd.DataFrame(wfm, index=orig_networks, columns=orig_networks)\n",
    "    wfm_df = wfm_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=0)\n",
    "    wfm_df = wfm_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=1)\n",
    "    \n",
    "    return wfm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model, Epoch as input, save weight feature map\n",
    "\n",
    "def visualize_wfm(trained_model,epoch,epoch_gap = 10,threshold=False,mode = 'sum'):\n",
    "    \n",
    "    if epoch%epoch_gap !=0:\n",
    "        return\n",
    "    \n",
    "    wfm_save_dir = outer_save_dir+'/WFM/'\n",
    "    if not os.path.isdir(wfm_save_dir):\n",
    "        os.mkdir(wfm_save_dir)\n",
    "    \n",
    "    w_ext = []\n",
    "    w_reg = []\n",
    "    w_dsc = []\n",
    "\n",
    "    ext_hidden1 = trained_model.ext_1.weight.shape[0]\n",
    "    prd_hidden1 = trained_model.prd_1.weight.shape[0]\n",
    "\n",
    "    w = []\n",
    "\n",
    "    for name,params in trained_model.named_parameters():\n",
    "        if \"weight\" in name and \"bn\" not in name:\n",
    "            w.append(params)\n",
    "\n",
    "    w_ext_1 = w[0].detach().cpu().numpy().T\n",
    "    w_reg_1 = w[1].detach().cpu().numpy().T\n",
    "    w_reg_2 = w[2].detach().cpu().numpy().T\n",
    "\n",
    "    temp_w_ext = w_ext_1\n",
    "    temp_w_reg = np.matmul(np.matmul(temp_w_ext, w_reg_1), w_reg_2)\n",
    "\n",
    "    w_ext.append(temp_w_ext) #ext x pred1 (352x1024)\n",
    "    w_reg.append(temp_w_reg) #ext x pred1 x pred2 (352x1)\n",
    "\n",
    "#     np.savez(wfm_save_dir+\"/wfm_reg_epoch{}\".format(epoch), X=w_reg)\n",
    "    ### ROI level interpretation - HJD ###\n",
    "    wfm_rois = w_reg[0].reshape(-1,)\n",
    "    wfm_rois[np.where(np.abs(stats.zscore(wfm_rois))<3.091)[0]] = 0\n",
    "    wfm_rois_df = make_wfm(wfm_rois)\n",
    "    \n",
    "    sns.set(style=\"white\", font_scale=3)\n",
    "    fig, ax = plt.subplots(figsize=(32, 32))\n",
    "    cbar_kws = dict(use_gridspec=False, shrink=0.85, location=\"right\")\n",
    "\n",
    "    sns.heatmap(\n",
    "        wfm_rois_df, square=True, cmap=\"RdBu_r\", center=0,  \n",
    "        ax=ax, cbar_kws=cbar_kws\n",
    "    )\n",
    "    ax.set_title(\"99.9% significant ROIs\")\n",
    "    ax.set_xticks(network_mid_idx)\n",
    "    ax.set_yticks(network_mid_idx)\n",
    "\n",
    "    ax.set_xticklabels(sorted_order, rotation=90, fontsize=45, ha=\"center\")\n",
    "    ax.set_yticklabels(sorted_order, rotation='horizontal', fontsize=45)\n",
    "\n",
    "    for network_pos in next_network_idx:\n",
    "        plt.axvline(network_pos, linewidth=1.5, color=\"black\", ymin=0, ymax=network_pos)\n",
    "        plt.axhline(network_pos, linewidth=1.5, color=\"black\", xmin=0, xmax=network_pos)\n",
    "\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_label(\"$WF$\", fontsize=60, labelpad=50)    \n",
    "    plt.savefig(wfm_save_dir+f\"/WFM_ROIs_epoch{epoch}.jpg\")\n",
    "    plt.close(fig)\n",
    "    ######\n",
    "    \n",
    "    wfm_df = make_wfm(stats.zscore(w_reg[0].reshape(-1,)))\n",
    "\n",
    "    cols = rows = wfm_df.columns.values\n",
    "#     new_orig_network_idx_order = np.unique(cols)\n",
    "    avg_df_1 = pd.DataFrame(columns=cols, index=rows)\n",
    "    avg_df_2 = pd.DataFrame(columns=new_orig_network_idx_order, index=new_orig_network_idx_order)\n",
    "    avg_df_3 = pd.DataFrame(columns=new_orig_network_idx_order, index=new_orig_network_idx_order)\n",
    "\n",
    "    for i, temp_row in enumerate(new_orig_network_idx_order):\n",
    "        for j, temp_col in enumerate(new_orig_network_idx_order):\n",
    "            temp_row_ids = np.where(rows == temp_row)[0]\n",
    "            temp_col_ids = np.where(cols == temp_col)[0]\n",
    "            temp_mat = wfm_df.iloc[temp_row_ids, temp_col_ids]\n",
    "            temp_vec = temp_mat.values.ravel()\n",
    "            n_temp_vec = np.sum(np.absolute(temp_mat.values) > 1.96)\n",
    "            if mode == 'sum':\n",
    "                sum_val = temp_vec.sum()\n",
    "                n_val = n_temp_vec\n",
    "            elif mode == 'avg':\n",
    "                if temp_row == temp_col:\n",
    "                    sum_val = temp_vec.sum() / (len(temp_row_ids) * (len(temp_row_ids) - 1))\n",
    "                    n_val = n_temp_vec / (len(temp_row_ids) * (len(temp_row_ids) - 1))\n",
    "                else:\n",
    "                    sum_val = temp_vec.sum() / (len(temp_row_ids) * (len(temp_col_ids)))\n",
    "                    n_val = n_temp_vec / (len(temp_row_ids) * (len(temp_col_ids)))\n",
    "            avg_df_1.iloc[temp_row_ids, temp_col_ids] = sum_val\n",
    "            avg_df_2.iloc[i, j] = sum_val\n",
    "            avg_df_3.iloc[i, j] = n_val\n",
    "\n",
    "    if threshold==False:\n",
    "        avg_mat = np.array(avg_df_2.values, dtype=np.float)\n",
    "        avg_mat = (avg_mat - avg_mat.mean()) / avg_mat.std()\n",
    "        avg_mat_df = pd.DataFrame(avg_mat, columns=new_orig_network_idx_order, index=new_orig_network_idx_order)\n",
    "        avg_mat_df = avg_mat_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=0)\n",
    "        avg_mat_df = avg_mat_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=1)\n",
    "        thr_zero = 0\n",
    "        avg_mat_df[(avg_mat_df < thr_zero) & (avg_mat_df > -thr_zero)] = 0\n",
    "\n",
    "        sns.set(style=\"white\", font_scale=4.5)\n",
    "        fig, ax = plt.subplots(figsize=(32, 32))\n",
    "        threshold = np.abs(avg_mat_df.values).max()\n",
    "        threshold = 5.5\n",
    "        cbar_kws = dict(\n",
    "            use_gridspec=False, shrink=0.85, location=\"right\", label=\"Correlation ($r$)\")\n",
    "        sns.heatmap(\n",
    "            avg_mat_df, square=True, cmap=\"vlag\", ax=ax, \n",
    "            mask=np.triu(np.ones(avg_mat_df.shape), 1).astype(np.bool),\n",
    "            vmax=threshold, vmin=-threshold, \n",
    "            linewidths=5, \n",
    "            cbar=True, cbar_kws=cbar_kws,\n",
    "            fmt=\".2f\", annot=True, annot_kws={\"fontsize\": 32}\n",
    "        )\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.set_label(\"$WF$\", fontsize=60, labelpad=50)\n",
    "        ax.set_xticklabels(new_orig_network_order, rotation=90, fontsize=60)\n",
    "        ax.set_yticklabels(new_orig_network_order, rotation=0, fontsize=60)\n",
    "        plt.title(f\"Epoch {epoch}/{epochs}\")\n",
    "        plt.savefig(wfm_save_dir+f\"/{mode}_WFM_epoch{epoch}.jpg\")\n",
    "        plt.close(fig)\n",
    "    #     plt.show()\n",
    "\n",
    "    if threshold==True:\n",
    "        avg_mat = np.array(avg_df_2.values, dtype=np.float)\n",
    "        avg_mat = (avg_mat - avg_mat.mean()) / avg_mat.std()\n",
    "        avg_mat_df = pd.DataFrame(avg_mat, columns=new_orig_network_idx_order, index=new_orig_network_idx_order)\n",
    "        avg_mat_df = avg_mat_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=0)\n",
    "        avg_mat_df = avg_mat_df.sort_index(key=lambda x: x.map(new_orig_network_order), axis=1)\n",
    "\n",
    "        annot = np.vectorize(lambda x: '' if np.absolute(x) < 3.091 else str(round(x, 2)))(avg_mat_df.to_numpy())\n",
    "        # annot = np.vectorize(lambda x: '' if np.absolute(x) < 2.58 else str(round(x, 2)))(avg_mat_df.to_numpy())\n",
    "        # annot = np.vectorize(lambda x: '' if np.absolute(x) < 1.96 else str(round(x, 2)))(avg_mat_df.to_numpy())\n",
    "\n",
    "        thr_zero = 0\n",
    "        avg_mat_df[(avg_mat_df < thr_zero) & (avg_mat_df > -thr_zero)] = 0\n",
    "\n",
    "        sns.set(style=\"white\", font_scale=5)\n",
    "        plt.rcParams['mathtext.fontset'] = 'custom'\n",
    "        plt.rcParams['mathtext.it'] = 'Arial:italic'\n",
    "        plt.rcParams['mathtext.rm'] = 'Arial'\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(32, 32))\n",
    "        threshold = np.abs(avg_mat_df.values).max()\n",
    "        threshold = 7\n",
    "        cbar_kws = dict(\n",
    "            use_gridspec=False, shrink=0.85, location=\"right\", label=\"Correlation ($r$)\")\n",
    "        sns.heatmap(\n",
    "            avg_mat_df, square=True, cmap=\"vlag\",\n",
    "            ax=ax,\n",
    "            mask=np.triu(np.ones(avg_mat_df.shape), 1).astype(np.bool),\n",
    "            vmax=threshold, vmin=-threshold, \n",
    "            linewidths=5, \n",
    "            cbar=True, cbar_kws=cbar_kws,\n",
    "            fmt=\"\", annot=annot, annot_kws={\"fontsize\": 36}\n",
    "        )\n",
    "        cbar = ax.collections[0].colorbar\n",
    "        cbar.set_label(\"$WF$\", fontsize=60, labelpad=50)\n",
    "        ax.set_xticklabels(new_orig_network_order, rotation=90, fontsize=60)\n",
    "        ax.set_yticklabels(new_orig_network_order, rotation=0, fontsize=60)\n",
    "        plt.title(f\"Epoch {epoch}/{epochs}\")\n",
    "        plt.savefig(wfm_save_dir+f\"/{mode}_WFM_epoch{epoch}.jpg\")\n",
    "        plt.close(fig)\n",
    "    #     plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_result(epoch,pearsonr,valid_prediction,valid_true):\n",
    "    \n",
    "    pred_save_dir = outer_save_dir+'/Prediction/'\n",
    "    if not os.path.isdir(pred_save_dir):\n",
    "        os.mkdir(pred_save_dir)\n",
    "    \n",
    "    sns.set(style=\"darkgrid\", font_scale=2)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    fig = sns.scatterplot(valid_true.flatten(), valid_prediction.flatten())\n",
    "    plt.title(f\"Epoch {epoch}/{epochs}, Pearson's r : %.3f\" % pearsonr)\n",
    "    plt.xlabel(\"True $p$-factor\")\n",
    "    plt.ylabel(\"Predicted $p$-factor\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylim(valid_prediction.min(),valid_prediction.max())\n",
    "    plt.savefig(pred_save_dir+f\"/Prediction_epoch{epoch}.png\")\n",
    "    plt.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make dataset & Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters for training\n",
    "mode = \"max\"\n",
    "min_lr = 1e-08\n",
    "lr_alpha = -1.5\n",
    "lr_beta = 1.7\n",
    "\n",
    "momentum = 0.90\n",
    "l1_param = 0\n",
    "early_stopping_patience = 20\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "n_classes = len(np.unique(y[:, scanner_idx]))\n",
    "output_prd_dim = 1\n",
    "output_dsc_dim = n_classes\n",
    "\n",
    "#Hyperparameter for sparsity control\n",
    "wsc_flag = [0, 0, 0]\n",
    "beta_lr = [1e-3, 2e-3, 2e-3] #1e-04, 1e-03, 1e-03\n",
    "max_beta = [1e-2, 2e-02, 2e-02] #1e-2, 5e-02, 5e-02\n",
    "n_wsc = wsc_flag.count(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dataset\n",
    "class MakeDataset(Dataset): \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        X_data = torch.from_numpy(self.X_data[idx]).type(torch.FloatTensor)\n",
    "        y_data = torch.from_numpy(self.y_data[idx]).type(torch.FloatTensor)\n",
    "\n",
    "        return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient reversal function (Ganin,2015)\n",
    "class GradRevFunc(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        lambda_ = ctx.lambda_\n",
    "        lambda_ = grads.new_tensor(lambda_)\n",
    "        dx = lambda_ * grads.neg()\n",
    "        return dx, None\n",
    "    \n",
    "class GradRev(torch.nn.Module):\n",
    "    def __init__(self, lambda_=0.0):\n",
    "        super(GradRev, self).__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradRevFunc.apply(x, self.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build SGNN model\n",
    "class SGNN(nn.Module):\n",
    "    def __init__(self, ext_hidden, prd_hidden, dsc_hidden,\n",
    "                 dropout_ext, dropout_prd, dropout_dsc, act_func_name, lambda_):\n",
    "        super(SGNN, self).__init__()\n",
    "        self.ext_1 = nn.Linear(input_dim, ext_hidden)\n",
    "        self.ext_bn_1 = nn.BatchNorm1d(ext_hidden)\n",
    "        \n",
    "        self.prd_1 = nn.Linear(ext_hidden, prd_hidden)\n",
    "        self.prd_bn_1 = nn.BatchNorm1d(prd_hidden)\n",
    "        self.prd_2 = nn.Linear(prd_hidden, output_prd_dim)\n",
    "        \n",
    "        self.dsc_1 = nn.Linear(ext_hidden, dsc_hidden)\n",
    "        self.dsc_bn_1 = nn.BatchNorm1d(dsc_hidden)\n",
    "        self.dsc_2 = nn.Linear(dsc_hidden, output_dsc_dim)\n",
    "\n",
    "        self.dropout_ext = nn.Dropout(p=dropout_ext)\n",
    "        self.dropout_prd = nn.Dropout(p=dropout_prd)\n",
    "        self.dropout_dsc = nn.Dropout(p=dropout_dsc)\n",
    "        \n",
    "        self.act_func = get_act_func(act_func_name)\n",
    "        self.GradRev = GradRev(lambda_)\n",
    "        self.weights_init()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_ftr = self.ext_1(x)\n",
    "        x_ftr = self.ext_bn_1(x_ftr)\n",
    "        x_ftr = self.act_func(x_ftr)\n",
    "        x_ftr = self.dropout_ext(x_ftr)\n",
    "        \n",
    "        x_prd = self.prd_1(x_ftr)\n",
    "        x_prd = self.prd_bn_1(x_prd)\n",
    "        x_prd = self.act_func(x_prd)\n",
    "        x_prd = self.dropout_prd(x_prd)\n",
    "        x_prd = self.prd_2(x_prd)\n",
    "        \n",
    "        x_rev = self.GradRev(x_ftr)\n",
    "        x_dsc = self.dsc_1(x_rev)\n",
    "        x_dsc = self.dsc_bn_1(x_dsc)\n",
    "        x_dsc = self.act_func(x_dsc)\n",
    "        x_dsc = self.dropout_dsc(x_dsc)\n",
    "        x_dsc = self.dsc_2(x_dsc)\n",
    "        \n",
    "        return x_prd, x_dsc\n",
    "    \n",
    "    def weights_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "                nn.init.normal_(m.bias, std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, opt_name, learning_rate=None, l2_param=None):\n",
    "    lower_opt_name = opt_name.lower()\n",
    "    if lower_opt_name == 'momentum':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                         momentum=momentum, weight_decay=l2_param)\n",
    "    elif lower_opt_name == 'nag':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                         momentum=momentum, weight_decay=l2_param, nesterov=True)\n",
    "    elif lower_opt_name == 'adam':\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate, \n",
    "                          weight_decay=l2_param)\n",
    "    elif lower_opt_name == 'sparseadam':\n",
    "        return optim.SparseAdam(model.parameters(), lr=learning_rate,\n",
    "                       betas=(0.9, 0.999), eps=1e-08, maximize=False)\n",
    "    elif lower_opt_name == 'radam':\n",
    "        return optim.RAdam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999),\n",
    "                           eps=1e-08, weight_decay=l2_param)\n",
    "    elif lower_opt_name == 'nadam':\n",
    "        return optim.NAdam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08,\n",
    "                  weight_decay=l2_param, momentum_decay=0.004)\n",
    "    elif lower_opt_name == 'adamax':\n",
    "        return optim.Adamax(model.parameters(), lr=learning_rate, betas=(0.9, 0.999),\n",
    "                            eps=1e-08, weight_decay=l2_param)\n",
    "    else:\n",
    "        sys.exit(\"Illegal arguement for optimizer type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_act_func(act_func_name):\n",
    "    act_func_name = act_func_name.lower()\n",
    "    if act_func_name == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif act_func_name == 'prelu':\n",
    "        return nn.PReLU()\n",
    "    elif act_func_name == 'elu':\n",
    "        return nn.ELU()\n",
    "    elif act_func_name == 'silu':\n",
    "        return nn.SiLU()\n",
    "    elif act_func_name == 'leakyrelu':\n",
    "        return nn.LeakyReLU()\n",
    "    elif act_func_name == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    elif act_func_name == 'selu':\n",
    "        return nn.SELU()\n",
    "    elif act_func_name == 'gelu':\n",
    "        return nn.GELU()\n",
    "    else:\n",
    "        sys.exit(\"Illegal arguement for activation function type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hsp(n_wsc, epochs):\n",
    "    hsp_val = torch.zeros(n_wsc)\n",
    "    beta_val = torch.clone(hsp_val)\n",
    "    hsp_list = torch.zeros((n_wsc, epochs))\n",
    "    beta_list = torch.zeros((n_wsc, epochs))\n",
    "    \n",
    "    return hsp_val, beta_val, hsp_list, beta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight sparsity control with Hoyer's sparsness (Layer wise)\n",
    "def calc_hsp(w, beta, max_beta, beta_lr, tg_hsp):\n",
    "    \n",
    "    # Get value of weight\n",
    "    [dim, n_nodes] = w.shape\n",
    "    num_elements = dim * n_nodes\n",
    "    norm_ratio = torch.norm(w.detach(), 1) / torch.norm(w.detach(), 2)\n",
    "\n",
    "    # Calculate hoyer's sparsity level\n",
    "    num = math.sqrt(num_elements) - norm_ratio\n",
    "    den = math.sqrt(num_elements) - 1\n",
    "    hsp = torch.tensor(num / den).to(device)\n",
    "\n",
    "    # Update beta\n",
    "    beta = beta.clone() + beta_lr * torch.sign(torch.tensor(tg_hsp).to(device) - hsp)\n",
    "    \n",
    "    # Trim value\n",
    "    beta = 0 if beta < 0 else beta\n",
    "    beta = max_beta if beta > max_beta else beta\n",
    "\n",
    "    return [hsp, beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_l1(model, epoch, hsp_val, beta_val, hsp_list, beta_list, tg_hsp):\n",
    "    l1_reg = None\n",
    "    layer_idx = 0\n",
    "    wsc_idx = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and \"bn\" not in name:\n",
    "            if \"ext\" in name or \"prd_1\" in name or \"dsc_1\" in name:\n",
    "                temp_w = param\n",
    "                \n",
    "                if wsc_flag[layer_idx] != 0:\n",
    "                    hsp_val[wsc_idx], beta_val[wsc_idx] = calc_hsp(\n",
    "                        temp_w, beta_val[wsc_idx], max_beta[wsc_idx], \n",
    "                        beta_lr[wsc_idx], tg_hsp[wsc_idx]\n",
    "                    )\n",
    "                    hsp_list[wsc_idx, epoch - 1] = hsp_val[wsc_idx]\n",
    "                    beta_list[wsc_idx, epoch - 1] = beta_val[wsc_idx]\n",
    "                    layer_reg = torch.norm(temp_w, 1) * beta_val[wsc_idx].clone()\n",
    "                    wsc_idx += 1\n",
    "                else:\n",
    "                    layer_reg = torch.norm(temp_w, 1) * l1_param\n",
    "\n",
    "                if l1_reg is None:\n",
    "                    l1_reg = layer_reg\n",
    "                else:\n",
    "                    l1_reg = l1_reg + layer_reg\n",
    "                layer_idx += 1\n",
    "        \n",
    "    return l1_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pearsonr(x, y):\n",
    "    x_mean = torch.mean(x.detach())\n",
    "    y_mean = torch.mean(y.detach())\n",
    "    xx = x.sub(x_mean)\n",
    "    yy = y.sub(y_mean)\n",
    "    num = xx.dot(yy)\n",
    "    den = torch.norm(xx, 2) * torch.norm(yy, 2)\n",
    "    corr = num / den\n",
    "    return corr.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mae(x, y):\n",
    "    return torch.abs(x - y).mean().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, train_loader, optimizer, criterion_prd, criterion_dsc, \n",
    "          hsp_val, beta_val, hsp_list, beta_list, tg_hsp, lambda_, X_train, y_train):\n",
    "    seed_everything(seed)\n",
    "    model.train()\n",
    "    prd_loss = 0\n",
    "    dsc_loss = 0\n",
    "    dsc_acc = 0\n",
    "    cost = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    y_train_true = []\n",
    "    y_train_pred = []\n",
    "    \n",
    "    for batch_idx, (input, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        output_prd, output_dsc = model(input)\n",
    "        \n",
    "        # 1. p-factor predictor loss\n",
    "        target_prd = target[:, p_factor_idx].view(-1, 1)\n",
    "        running_prd_loss = criterion_prd(output_prd, target_prd)\n",
    "        l1_norm = calc_l1(model, epoch, hsp_val, beta_val, hsp_list, beta_list, tg_hsp)\n",
    "\n",
    "        if epoch > pretrain_epoch:\n",
    "            # Undersampling for scanner-generalization\n",
    "            target_dsc = target[:, scanner_idx].long().view(-1)\n",
    "            scnr_smp = target[:, scanner_idx].detach().cpu().numpy()\n",
    "            n_minor = (scnr_smp == 0).sum()\n",
    "            n_major = len(scnr_smp) - n_minor\n",
    "            if n_minor != 0 and n_major != 0:\n",
    "                minor_idx = np.where(scnr_smp == 0)[0]\n",
    "                major_idx = np.where(scnr_smp != 0)[0]\n",
    "                major_smp_idx = np.random.choice(major_idx, size=n_minor, replace=True)\n",
    "                smp_idx = np.concatenate((minor_idx.astype(np.int), major_smp_idx.astype(np.int)))\n",
    "                running_dsc_loss = criterion_dsc(output_dsc[smp_idx], target_dsc[smp_idx])\n",
    "                dsc_loss += running_dsc_loss.detach()\n",
    "            else:\n",
    "                running_dsc_loss = 0\n",
    "                dsc_loss += 0\n",
    "\n",
    "            # Total Loss\n",
    "            running_loss = running_dsc_loss + running_prd_loss + l1_norm.clone()\n",
    "            \n",
    "        else:\n",
    "            running_loss = running_prd_loss + l1_norm.clone()\n",
    "\n",
    "        cost = running_loss\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        prd_loss += running_prd_loss.detach()\n",
    "        total += output_prd.size(0)\n",
    "        true_batch = torch.flatten(target_prd.detach())\n",
    "        pred_batch = torch.flatten(output_prd.detach())\n",
    "        y_train_true.append(true_batch)\n",
    "        y_train_pred.append(pred_batch)\n",
    "        \n",
    "    X_train = torch.from_numpy(X_train).type(torch.FloatTensor).to(device)\n",
    "    _, output_dsc = model(X_train)\n",
    "    _, scnr_pred = torch.max(output_dsc.data, 1)\n",
    "    scnr_pred = scnr_pred.detach().cpu().numpy().ravel()\n",
    "    scnr_true = y_train[:, scanner_idx].ravel()\n",
    "    dsc_acc = balanced_accuracy_score(scnr_true, scnr_pred)\n",
    "    \n",
    "    prd_loss /= total\n",
    "    dsc_loss /= total\n",
    "    y_train_true = torch.flatten(torch.stack(y_train_true))\n",
    "    y_train_pred = torch.flatten(torch.stack(y_train_pred))\n",
    "    train_corr = calc_pearsonr(y_train_true, y_train_pred)\n",
    "    train_mae = calc_mae(y_train_true, y_train_pred).detach().cpu().numpy()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "#     plot_prediction_result(epoch,train_corr,y_train_pred.detach().cpu().numpy(),\n",
    "#                            y_train_true.detach().cpu().numpy())\n",
    "\n",
    "    return prd_loss, dsc_loss, dsc_acc, train_corr, train_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, epoch, val_loader, criterion_prd, criterion_dsc, X_val, y_val):\n",
    "    seed_everything(seed)\n",
    "    model.eval()\n",
    "    prd_loss = 0\n",
    "    total = 0\n",
    "    y_val_true = []\n",
    "    y_val_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input, target in val_loader:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            output_prd, output_clf = model(input)\n",
    "            target_prd = target[:, p_factor_idx].view(-1, 1)\n",
    "            running_prd_loss = criterion_prd(output_prd, target_prd)\n",
    "            prd_loss += running_prd_loss.detach()\n",
    "            total += output_prd.size(0)\n",
    "            true_batch = torch.flatten(target_prd.detach())\n",
    "            pred_batch = torch.flatten(output_prd.detach())\n",
    "            y_val_true.append(true_batch)\n",
    "            y_val_pred.append(pred_batch)\n",
    "\n",
    "    X_val = torch.from_numpy(X_val).type(torch.FloatTensor).to(device)\n",
    "    _, output_dsc = model(X_val)\n",
    "    _, scnr_pred = torch.max(output_dsc.data, 1)\n",
    "    scnr_pred = scnr_pred.detach().cpu().numpy().ravel()\n",
    "    scnr_true = y_val[:, scanner_idx].ravel()\n",
    "    dsc_acc = balanced_accuracy_score(scnr_true, scnr_pred)\n",
    "\n",
    "    y_val_true = torch.flatten(torch.stack(y_val_true))\n",
    "    y_val_pred = torch.flatten(torch.stack(y_val_pred))\n",
    "    val_corr = calc_pearsonr(y_val_true, y_val_pred)\n",
    "    val_mae = calc_mae(y_val_true, y_val_pred).detach().cpu().numpy()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "#     plot_prediction_result(epoch,val_corr,y_val_pred.detach().cpu().numpy(),\n",
    "#                            y_val_true.detach().cpu().numpy())\n",
    "\n",
    "    return prd_loss, val_corr, val_mae, dsc_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch, test_loader, criterion_prd, criterion_dsc, X_test, y_test):\n",
    "    seed_everything(seed)\n",
    "    model.eval()\n",
    "    prd_loss = 0\n",
    "    total = 0\n",
    "    y_test_true = []\n",
    "    y_test_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input, target in test_loader:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            output_prd, output_clf = model(input)\n",
    "            target_prd = target[:, p_factor_idx].view(-1, 1)\n",
    "            running_prd_loss = criterion_prd(output_prd, target_prd)\n",
    "            prd_loss += running_prd_loss.detach()\n",
    "            total += output_prd.size(0)\n",
    "            true_batch = torch.flatten(target_prd.detach())\n",
    "            pred_batch = torch.flatten(output_prd.detach())\n",
    "            y_test_true.append(true_batch)\n",
    "            y_test_pred.append(pred_batch)\n",
    "\n",
    "    X_test = torch.from_numpy(X_test).type(torch.FloatTensor).to(device)\n",
    "    _, output_dsc = model(X_test)\n",
    "    _, scnr_pred = torch.max(output_dsc.data, 1)\n",
    "    scnr_pred = scnr_pred.detach().cpu().numpy().ravel()\n",
    "    scnr_true = y_test[:, scanner_idx].ravel()\n",
    "    dsc_acc = balanced_accuracy_score(scnr_true, scnr_pred)\n",
    "\n",
    "    y_test_true = torch.flatten(torch.stack(y_test_true))\n",
    "    y_test_pred = torch.flatten(torch.stack(y_test_pred))\n",
    "    test_corr = calc_pearsonr(y_test_true, y_test_pred)\n",
    "    test_mae = calc_mae(y_test_true, y_test_pred).detach().cpu().numpy()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    plot_prediction_result(epoch,test_corr,y_test_pred.detach().cpu().numpy(),\n",
    "                           y_test_true.detach().cpu().numpy())\n",
    "\n",
    "    return prd_loss, test_corr, test_mae, dsc_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"주어진 patience 이후로 validation loss가 개선되지 않으면 학습을 조기 중지\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): validation loss가 개선된 후 기다리는 기간\n",
    "                            Default: 7\n",
    "            verbose (bool): True일 경우 각 validation loss의 개선 사항 메세지 출력\n",
    "                            Default: False\n",
    "            delta (float): 개선되었다고 인정되는 monitered quantity의 최소 변화\n",
    "                            Default: 0\n",
    "            path (str): checkpoint저장 경로\n",
    "                            Default: 'checkpoint.pt'\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''validation loss가 감소하면 모델을 저장한다.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(\n",
    "    save_dir, epochs, train_loss, val_loss, test_loss,\n",
    "    train_corr, val_corr, test_corr,train_acc_scd, val_acc_scd,test_acc_scd,\n",
    "    lr, plot_hsp_list, plot_beta_list, tg_hsp):\n",
    "    \n",
    "    sns.set(style=\"darkgrid\", font_scale=2)\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(20, 10))\n",
    "    ax = ax.flat\n",
    "    lw = 4\n",
    "    last_epoch = epochs\n",
    "    \n",
    "    #Loss plot\n",
    "    ax[0].plot(train_loss[:last_epoch], label='train loss', lw=lw, color=\"g\")\n",
    "    ax[0].tick_params(axis='y',labelcolor='g')\n",
    "    ax_01 = ax[0].twinx()\n",
    "\n",
    "    ax_01.plot(val_loss[:last_epoch], label='val loss', lw=lw, color=\"orange\")\n",
    "    ax_01.tick_params(axis='y',labelcolor='orange')\n",
    "    ax_01.plot(test_loss[:last_epoch], label='test loss', lw=lw, color=\"r\")\n",
    "    ax_01.tick_params(axis='y',labelcolor='r')\n",
    "    \n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(\"Loss Plot\", pad=10)\n",
    "\n",
    "    #Corr plot\n",
    "    ax[1].plot(train_corr[:last_epoch], label='train corr', lw=lw, color=\"g\")\n",
    "    ax[1].tick_params(axis='y',labelcolor='g')\n",
    "    ax_02 = ax[1].twinx()\n",
    "\n",
    "    ax_02.plot(val_corr[:last_epoch], label='val corr', lw=lw, color=\"orange\")\n",
    "    ax_02.tick_params(axis='y',labelcolor='orange')\n",
    "    ax_02.plot(test_corr[:last_epoch], label='test corr', lw=lw, color=\"r\")\n",
    "    ax_02.tick_params(axis='y',labelcolor='r')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_title(\"Correlation Plot\", pad=10)\n",
    "\n",
    "    plot_hsp_list, plot_beta_list = np.array(plot_hsp_list).T, np.array(plot_beta_list).T\n",
    "    \n",
    "    #hsp plot\n",
    "    for idx, n_layer in enumerate(indices):\n",
    "        ax[3].plot(plot_hsp_list[idx], label='layer{}'.format(n_layer), lw=lw)\n",
    "        ax[4].plot(plot_beta_list[idx], \n",
    "                   label='layer{}'.format(n_layer), lw=lw)\n",
    "        ax[3].legend(); ax[4].legend()\n",
    "        ax[3].set_title(\"HSP plot [{:.3f}/{:.3f}]\"\n",
    "                        .format(plot_hsp_list[0, -1], tg_hsp[0][0]), pad=10)\n",
    "        ax[4].set_title(\"Beta plot\", pad=10)\n",
    "    \n",
    "    #Scanner acc plot\n",
    "    ax[2].plot(train_acc_scd[:last_epoch], label='train acc', lw=lw, color=\"g\")\n",
    "    ax[2].plot(val_acc_scd[:last_epoch], label='val acc', lw=lw, color=\"orange\")\n",
    "    ax[2].plot(test_acc_scd[:last_epoch], label='test acc', lw=lw, color=\"r\")\n",
    "    ax[2].set_title(\"Acc plot - scanner\")\n",
    "    ax[2].legend()\n",
    "    \n",
    "    #Learning rate plot\n",
    "    ax[5].plot(lr[:last_epoch],label='Learning rate',lw=lw,color='m')\n",
    "    ax[5].set_title(\"Learning rate\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"{}/Learning_curves.png\".format(save_dir))\n",
    "    \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_outer_fold(n_outer_cv=0, outer_save_dir=None, sel_tg_hsp=None,sel_lambda=None):\n",
    "    seed_everything(seed)\n",
    "    outer_cv_list = []\n",
    "    \n",
    "    # Outer fold\n",
    "    print(\"\\n===================================\", end=\" \")\n",
    "    print(\"Outer Fold [{}/{}]\".format(n_outer_cv + 1, outer_n_splits), end=\" \")\n",
    "    print(\"===================================\")\n",
    "    \n",
    "    outer_start_fold_time = time.time()\n",
    "    outer_train_idx = outer_train_folds_idx[n_outer_cv]\n",
    "    outer_val_idx = outer_val_folds_idx[n_outer_cv]\n",
    "    outer_test_idx = outer_test_folds_idx[n_outer_cv]\n",
    "\n",
    "    X_train, y_train = X[outer_train_idx], y[outer_train_idx]\n",
    "    X_val, y_val = X[outer_val_idx], y[outer_val_idx]\n",
    "    X_test, y_test = X[outer_test_idx], y[outer_test_idx]\n",
    "    \n",
    "    X_train = stats.zscore(X_train, axis=1)\n",
    "    X_val = stats.zscore(X_val, axis=1)\n",
    "    X_test = stats.zscore(X_test, axis=1)\n",
    "        \n",
    "    outer_train_dataset = MakeDataset(X_train, y_train)\n",
    "    outer_val_dataset = MakeDataset(X_val, y_val)\n",
    "    outer_test_dataset = MakeDataset(X_test, y_test)\n",
    "    \n",
    "    outer_train_loader = DataLoader(\n",
    "        outer_train_dataset, batch_size=batch_size, pin_memory=True,\n",
    "        shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "    outer_val_loader = DataLoader(\n",
    "        outer_val_dataset, batch_size=len(y_val), pin_memory=True,\n",
    "        shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "    outer_test_loader = DataLoader(\n",
    "        outer_test_dataset, batch_size=len(y_test), pin_memory=True,\n",
    "        shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "        \n",
    "    # Assign model \n",
    "    model = SGNN(\n",
    "        ext_hidden, dsc_hidden, prd_hidden, \n",
    "        dropout_ext, dropout_prd, dropout_dsc, act_func_name, sel_lambda\n",
    "    ).to(device)\n",
    "    optimizer = get_optimizer(model, optimizer_name, learning_rate, l2_param)\n",
    "    lr_factor = lr_alpha * sel_tg_hsp[0][0] + lr_beta\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=mode, patience=lr_patience, min_lr=min_lr, factor=lr_factor\n",
    "    )\n",
    "    cosine_scheduler = CosineAnnealingWarmRestarts(optimizer,20,eta_min = min_lr)\n",
    "    early_stopping = EarlyStopping(patience=early_stopping_patience,\n",
    "                                   path=outer_save_dir + \"/model_fold_\" + str(n_outer_cv + 1) + \".pt\")\n",
    "    es_switch_count = 0\n",
    "    es_switch = False\n",
    "    criterion_prd = nn.MSELoss()\n",
    "    criterion_dsc = nn.CrossEntropyLoss() # nn.BCELoss()\n",
    "              \n",
    "    # list to save learning parameters\n",
    "    outer_train_loss = []\n",
    "    outer_val_loss=[]\n",
    "    outer_test_loss = []\n",
    "    outer_train_corr = []\n",
    "    outer_val_corr = []\n",
    "    outer_test_corr = []\n",
    "    outer_train_acc = []\n",
    "    outer_val_acc = []\n",
    "    outer_test_acc = []\n",
    "    outer_train_mae = []\n",
    "    outer_val_mae = []\n",
    "    outer_test_mae = []\n",
    "    outer_lr = []\n",
    "    outer_hsp_list = []\n",
    "    outer_beta_list = []\n",
    "\n",
    "    hsp_val, beta_val, hsp_list, beta_list = init_hsp(n_wsc, epochs)\n",
    "        \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_prd_loss, train_dsc_loss, train_acc, train_corr, train_mae = train(\n",
    "            model, epoch, outer_train_loader, \n",
    "            optimizer, criterion_prd, criterion_dsc, \n",
    "            hsp_val, beta_val, hsp_list, beta_list, sel_tg_hsp, sel_lambda,\n",
    "            X_train, y_train\n",
    "        )\n",
    "        val_prd_loss, val_corr, val_mae, val_acc = valid(\n",
    "            model, epoch, outer_val_loader, criterion_prd, criterion_dsc, X_val, y_val\n",
    "        )\n",
    "        test_prd_loss, test_corr, test_mae, test_acc = test(\n",
    "            model, epoch, outer_test_loader, criterion_prd, criterion_dsc, X_test, y_test\n",
    "        )\n",
    "\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        outer_train_loss.append([train_prd_loss, train_dsc_loss])\n",
    "        outer_train_mae.append(train_mae)\n",
    "        outer_train_corr.append(train_corr)\n",
    "        outer_train_acc.append(train_acc)\n",
    "        outer_val_loss.append([val_prd_loss, []])\n",
    "        outer_val_mae.append(val_mae)\n",
    "        outer_val_corr.append(val_corr)\n",
    "        outer_val_acc.append(val_acc)\n",
    "        outer_test_loss.append([test_prd_loss, []])\n",
    "        outer_test_mae.append(test_mae)\n",
    "        outer_test_corr.append(test_corr)\n",
    "        outer_test_acc.append(test_acc)\n",
    "        outer_lr.append(lr)\n",
    "        outer_hsp_list.append(list(hsp_val.clone()))\n",
    "        outer_beta_list.append(list(beta_val.clone()))\n",
    "\n",
    "        if epoch % print_epoch == 0:\n",
    "            print(\"\\nEpoch [{:d}/{:d}]\".format(epoch, epochs), end=\" \")\n",
    "            print(\"Train corr: {:.4f}, Test corr: {:.4f}, Train loss: {:.4f}, Test loss: {:.4f}\"\n",
    "                  .format(train_corr, test_corr, train_prd_loss, test_prd_loss))\n",
    "            for i in range(len(wsc_flag)):\n",
    "                if wsc_flag[i] != 0:\n",
    "                    print(\"Layer {:d}: [{:.4f}/{:.4f}]\".\n",
    "                          format( i + 1, hsp_val[i], sel_tg_hsp[i][0]), end=\" \")\n",
    "            print(\"Train acc: {:.2f}\".format(train_acc))\n",
    "\n",
    "\n",
    "            plot_learning_curves(\n",
    "                outer_save_dir, epochs,\n",
    "                np.array(outer_train_mae), np.array(outer_val_mae), np.array(outer_test_mae),  \n",
    "                outer_train_corr,outer_val_corr, outer_test_corr,\n",
    "                outer_train_acc,outer_val_acc,outer_test_acc,\n",
    "                outer_lr, outer_hsp_list, outer_beta_list, sel_tg_hsp\n",
    "            )\n",
    "\n",
    "            \n",
    "        ###Debugging code_Tracing WFM during training -HJD###\n",
    "        if epoch % 5 ==0:\n",
    "            visualize_wfm(model,epoch,epoch_gap = 5,threshold=False,mode='sum')\n",
    "            visualize_wfm(model,epoch,epoch_gap = 5,threshold=False,mode='avg')\n",
    "        ######\n",
    "        if 'Full' in select_fold:\n",
    "            if len(hsp_val)>0:\n",
    "                if hsp_val[0] < temp_param['hsp_ext']:\n",
    "                    scheduler.step(hsp_val[0])\n",
    "        else:\n",
    "            if len(hsp_val)>0:\n",
    "                if hsp_val[0] < temp_param['hsp_ext']:\n",
    "                    scheduler.step(hsp_val[0])\n",
    "                if hsp_val[0] >= temp_param['hsp_ext'] and hsp_val[1] >= temp_param['hsp_prd'] and hsp_val[2] >= temp_param['hsp_dsc'] and epoch > pretrain_epoch:\n",
    "                    es_switch_count +=1\n",
    "                    if es_switch_count>5:\n",
    "                        es_switch = True\n",
    "                        cosine_scheduler.step()\n",
    "                if es_switch:\n",
    "                    early_stopping(val_prd_loss,model)\n",
    "                    if early_stopping.early_stop:\n",
    "                        print(\"Early stopping\")\n",
    "                        break\n",
    "            \n",
    "            \n",
    "#     train_prd_loss, train_corr, train_mae, train_acc = test(\n",
    "#         model, epoch, outer_train_loader, criterion_prd, criterion_dsc, X_train, y_train\n",
    "#     )\n",
    "\n",
    "    if 'Full' in select_fold:\n",
    "        torch.save(model.state_dict(), \n",
    "                   outer_save_dir + \"/model_fold_\" + str(n_outer_cv + 1) + \".pt\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    outer_cv_list.append([train_corr, val_corr, test_corr, train_mae, val_mae, test_mae,\n",
    "                          train_acc, val_acc, test_acc])\n",
    "            \n",
    "    outer_cv_df = pd.DataFrame(\n",
    "        np.array(outer_cv_list), \n",
    "        columns=[\"train_corr\", \"valid_corr\",\"test_corr\",\n",
    "                 \"train_mae\", \"valid_mae\", \"test_mae\",\"train_acc\", \"valid_acc\",\"test_acc\"]\n",
    "    )\n",
    "    outer_cv_df.to_csv(\"{}/outer_cv.csv\".format(outer_save_dir))\n",
    "\n",
    "    outer_tot_time = time.time() - outer_start_fold_time\n",
    "    print(\"\\nExecution Time for Fold: {:.2f} mins\".format(outer_tot_time / 60))\n",
    "    \n",
    "    return train_corr, test_corr, train_mae, test_mae, outer_train_acc, outer_train_corr, outer_hsp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Settle parameters, output path & Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = list(ParameterGrid(param_cand))\n",
    "\n",
    "temp_param = param_grid[temp_sel_idx]\n",
    "\n",
    "ext_hidden = temp_param[\"ext\"]\n",
    "prd_hidden = temp_param[\"prd\"]\n",
    "dsc_hidden = temp_param[\"dsc\"]\n",
    "\n",
    "dropout_ext = temp_param[\"dropout_ext\"]\n",
    "dropout_prd = temp_param[\"dropout_prd\"]\n",
    "dropout_dsc = temp_param[\"dropout_dsc\"]\n",
    "\n",
    "\n",
    "batch_size = temp_param[\"batch_size\"]\n",
    "learning_rate = temp_param[\"lr\"]\n",
    "epochs = temp_param[\"epochs\"]\n",
    "l2_param = temp_param[\"l2_param\"]\n",
    "pretrain_epoch = temp_param[\"pretrain_epoch\"]\n",
    "\n",
    "lr_patience = temp_param[\"lr_patience\"]\n",
    "# lr_factor = temp_param[\"lr_factor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/hjd/IG_my_study/SNUH/data/temp/p/Hsp:[0.0,0.0,0.0]_Maxb:[0.01, 0.02, 0.02]_Betalr:[0.001, 0.002, 0.002]_LR:[5e-05]_Act:[elu]_Opt:[nag]_DO:[0.9,0.9,0.0]_lambda:[0.01]_seed[42]\n"
     ]
    }
   ],
   "source": [
    "save_folder = f\"{target_name}/Hsp:[{temp_param['hsp_ext']*wsc_flag[0]},{temp_param['hsp_prd']*wsc_flag[1]},{temp_param['hsp_dsc']*wsc_flag[2]}]_Maxb:{max_beta}_Betalr:{beta_lr}_LR:[{temp_param['lr']}]_Act:[{act_func_name}]_Opt:[{optimizer_name}]_DO:[{temp_param['dropout_ext']},{temp_param['dropout_prd']},{temp_param['dropout_dsc']}]_lambda:[{temp_param['lambda_']}]_seed[{seed}]\"\n",
    "save_path = \"/users/hjd/IG_my_study/SNUH/data/temp/\"\n",
    "output_folder = os.path.join(save_path, save_folder)\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "elif \"Full\" in select_fold:\n",
    "    if \"Outer_fold_Full\" in os.listdir(output_folder):\n",
    "        print(output_folder)\n",
    "        raise Exception(\"Result already exist. Check directory!\")\n",
    "        \n",
    "elif len(os.listdir(output_folder))>0:\n",
    "    for fold in [f'Outer_fold_{i+1}' for i in select_fold]:\n",
    "        if fold in os.listdir(output_folder):\n",
    "            print(output_folder)\n",
    "            raise Exception(\"Result already exist. Check directory!\")\n",
    "    \n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/hjd/IG_my_study/SNUH/data/temp/p/Hsp:[0.0,0.0,0.0]_Maxb:[0.01, 0.02, 0.02]_Betalr:[0.001, 0.002, 0.002]_LR:[5e-05]_Act:[elu]_Opt:[nag]_DO:[0.9,0.9,0.0]_lambda:[0.01]_seed[42]\n",
      "\n",
      "=================================== Outer Fold [1/5] ===================================\n",
      "Selected param: hsp_dsc: 0.3 hsp_ext: 0.95 hsp_prd: 0.3 \n",
      "=================================== Outer Fold [1/5] ===================================\n",
      "\n",
      "Epoch [5/10] Train corr: 0.5830, Test corr: 0.1610, Train loss: 0.0065, Test loss: 0.3013\n",
      "Train acc: 0.51\n",
      "\n",
      "Epoch [10/10] Train corr: 0.8657, Test corr: 0.1504, Train loss: 0.0025, Test loss: 0.3017\n",
      "Train acc: 0.51\n",
      "\n",
      "Execution Time for Fold: 2.77 mins\n",
      "\n",
      "Outer Fold [1/5]: train corr: 0.8657, valid corr: 0.1504\n"
     ]
    }
   ],
   "source": [
    "seed_everything(seed)\n",
    "code_start_time = time.time()\n",
    "\n",
    "print(output_folder)\n",
    "\n",
    "outer_cv = []\n",
    "\n",
    "for n_outer_cv in outer_cv_part:\n",
    "    \n",
    "    if \"Full\" in select_fold:\n",
    "        pass\n",
    "    elif n_outer_cv not in select_fold:\n",
    "        continue\n",
    "        \n",
    "    print(\"\\n===================================\", end=\" \")\n",
    "    print(\"Outer Fold [{}/{}]\".format(n_outer_cv + 1, outer_n_splits), end=\" \")\n",
    "    print(\"===================================\")\n",
    "\n",
    "    outer_save_dir = \"{}/Outer_fold_{}\".format(output_folder, n_outer_cv + 1)\n",
    "    if \"Full\" in select_fold:\n",
    "        print(\"Training Full subjects!\")\n",
    "        outer_save_dir = \"{}/Outer_fold_Full\".format(output_folder)\n",
    "    os.makedirs(outer_save_dir, exist_ok=True)\n",
    "\n",
    "    sel_idx = temp_sel_idx\n",
    "    sel_param = param_grid[sel_idx]\n",
    "    sel_hsp = []\n",
    "    sel_lambda = sel_param[\"lambda_\"]\n",
    "    print(\"Selected param:\", end=\" \")\n",
    "    for x in sel_param:\n",
    "        if \"hsp\" in x: \n",
    "            print(\"{}: {}\".format(x, sel_param[x]), end=\" \")\n",
    "            sel_hsp.append(sel_param[x])\n",
    "    \n",
    "    # Outer Fold\n",
    "    hsp_cand_1 = [sel_param[\"hsp_ext\"]]\n",
    "    hsp_cand_2 = [sel_param[\"hsp_prd\"]]\n",
    "    hsp_cand_3 = [sel_param[\"hsp_dsc\"]]\n",
    "\n",
    "    indices = [i + 1 for i, x in enumerate(wsc_flag) if x == 1]\n",
    "    hsp_cand_list = list(itertools.product(hsp_cand_1, hsp_cand_2, hsp_cand_3))\n",
    "    hsp_cand_list = [list(i) for i in hsp_cand_list]\n",
    "    hsp_cand = [hsp_cand_1, hsp_cand_2, hsp_cand_3]\n",
    "    sel_tg_hsp = hsp_cand\n",
    "\n",
    "    (outer_train_corr, outer_test_corr, outer_train_mae, \n",
    "    outer_test_mae, outer_train_acc, outer_test_acc, outer_hsp_list) = run_outer_fold(\n",
    "        n_outer_cv, outer_save_dir, sel_tg_hsp, sel_lambda\n",
    "    )\n",
    "    \n",
    "    outer_cv.append([sel_hsp, outer_train_corr, outer_test_corr, outer_train_mae, outer_test_mae])\n",
    "    \n",
    "    print(\"\\nOuter Fold [{}/{}]: train corr: {:.4f}, valid corr: {:.4f}\"\n",
    "          .format(n_outer_cv + 1, outer_n_splits, outer_train_corr, outer_test_corr))\n",
    "    \n",
    "    ###Debugging code_Save WFM - HJD###f\n",
    "    path_avg = [i for i in os.listdir(outer_save_dir+'/WFM') if 'jpg' in i and 'avg' in i]\n",
    "    path_sum = [i for i in os.listdir(outer_save_dir+'/WFM') if 'jpg' in i and 'sum' in i]\n",
    "#     path2 = [i for i in path if int(i.split('epoch')[1].split('.jpg')[0])<=100]\n",
    "    imgs_avg = [Image.open(outer_save_dir+'/WFM/'+i) for i in path_avg]\n",
    "    imgs_sum = [Image.open(outer_save_dir+'/WFM/'+i) for i in path_sum]\n",
    "    imageio.mimsave(outer_save_dir+'/WM_changes_avg.gif', imgs_avg, fps=10)\n",
    "    imageio.mimsave(outer_save_dir+'/WM_changes_sum.gif', imgs_sum, fps=10)\n",
    "\n",
    "    ###Debugging code_Save scatterplot - HJD###\n",
    "    path = [i for i in os.listdir(outer_save_dir+'/Prediction') if 'png' in i]\n",
    "    imgs = [Image.open(outer_save_dir+'/Prediction/'+i) for i in path]\n",
    "    imageio.mimsave(outer_save_dir+'/Prediction_changes.gif', imgs, fps=10)\n",
    "    ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time for the training: 0.05 hours\n"
     ]
    }
   ],
   "source": [
    "code_tot_time = time.time() - code_start_time \n",
    "print(\"Execution Time for the training: {:.2f} hours\".format(code_tot_time / 60 / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.8657, Test: 0.1504\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(outer_cv, columns=[\"hsp\", \"train_corr\", \"test_corr\",\"train_mae\",\"test_mae\"])\n",
    "train_avg = np.array([x for x in df[\"train_corr\"].values]).mean()\n",
    "test_avg = np.array([x for x in df[\"test_corr\"].values]).mean()\n",
    "print(\"Train: {:.4f}, Test: {:.4f}\".format(train_avg, test_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size 32\n",
      "dropout_dsc 0.0\n",
      "dropout_ext 0.9\n",
      "dropout_prd 0.9\n",
      "dsc 1024\n",
      "epochs 10\n",
      "ext 1024\n",
      "hsp_dsc 0.3\n",
      "hsp_ext 0.95\n",
      "hsp_prd 0.3\n",
      "l2_param 0.05\n",
      "lambda_ 0.01\n",
      "lr 5e-05\n",
      "lr_factor 0.5\n",
      "lr_patience 5\n",
      "prd 1024\n",
      "pretrain_epoch 20\n"
     ]
    }
   ],
   "source": [
    "tmp_param_grid = list(ParameterGrid(param_cand))[temp_sel_idx]\n",
    "for name in tmp_param_grid:\n",
    "    print(name, tmp_param_grid[name])\n",
    "    \n",
    "import json\n",
    "with open(outer_save_dir+'/params.json', 'w') as fp:\n",
    "    json.dump(tmp_param_grid, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lr'] = learning_rate\n",
    "df['beta_lr'] = [beta_lr]*len(df)\n",
    "df['max_beta'] = [max_beta]*len(df)\n",
    "df['l2_param'] = l2_param\n",
    "df['batch_size'] = batch_size\n",
    "df['act_func'] = act_func_name\n",
    "df['optimizer'] = optimizer_name\n",
    "df['momentum'] = momentum\n",
    "if 'Full' in select_fold:\n",
    "    df['train_idx'] = [outer_train_folds_idx[0]]\n",
    "    df['test_idx'] = [outer_test_folds_idx[0]]\n",
    "    df.to_csv(outer_save_dir+f\"/result_df_{select_fold}.csv\", sep='\\t',index=None)\n",
    "else:\n",
    "    df['train_idx'] = [outer_train_folds_idx[i] for i in select_fold]\n",
    "    df['test_idx'] = [outer_test_folds_idx[i] for i in select_fold]\n",
    "    df.to_csv(outer_save_dir+f\"/result_df_{np.array(select_fold)+1}.csv\", sep='\\t',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
